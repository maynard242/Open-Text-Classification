{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'shared_lib.utils' from 'shared_lib/utils.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys, re, json, time, shutil\n",
    "import itertools, collections\n",
    "from IPython.display import display, HTML\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.1\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "# Import model\n",
    "#import cnnlm\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data_Set/'\n",
    "PROJECT_PATH = os.getcwd()\n",
    "PROJECT_DATA = os.path.join(PROJECT_PATH, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process 20 newsgroup data as input to CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load 20 newsgroup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 0\t = alt.atheism\n",
      "class: 1\t = comp.graphics\n",
      "class: 2\t = comp.os.ms-windows.misc\n",
      "class: 3\t = comp.sys.ibm.pc.hardware\n",
      "class: 4\t = comp.sys.mac.hardware\n",
      "class: 5\t = comp.windows.x\n",
      "class: 6\t = misc.forsale\n",
      "class: 7\t = rec.autos\n",
      "class: 8\t = rec.motorcycles\n",
      "class: 9\t = rec.sport.baseball\n",
      "class: 10\t = rec.sport.hockey\n",
      "class: 11\t = sci.crypt\n",
      "class: 12\t = sci.electronics\n",
      "class: 13\t = sci.med\n",
      "class: 14\t = sci.space\n",
      "class: 15\t = soc.religion.christian\n",
      "class: 16\t = talk.politics.guns\n",
      "class: 17\t = talk.politics.mideast\n",
      "class: 18\t = talk.politics.misc\n",
      "class: 19\t = talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Get newsgroup data\n",
    "newsgroup_data_all = fetch_20newsgroups(subset = 'all', remove=('headers', 'footers', 'quotes'))\n",
    "all_data, all_labels = newsgroup_data_all.data, newsgroup_data_all.target\n",
    "\n",
    "# List of all the class labels\n",
    "label_list = list(newsgroup_data_all.target_names)\n",
    "\n",
    "# Print the class labels\n",
    "i = 0\n",
    "for label in label_list:\n",
    "    print \"class: %i\\t = %s\" %(i, label)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Determine the length of document to use\n",
    "\n",
    "* From the distribution, must of the document is quite short. The original paper use **2000** word length which we can use this length or try other length if needed. \n",
    "\n",
    "* If a document is longer than the defined doc length, it will be cut, if it is shorter, it will be padded.\n",
    "\n",
    "* < PAD > tag is used for padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document length distribution of 20 newsgroup data\n",
    "\n",
    "* Does using 2000 document length makes sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Samples: 18846, Max Length: 158791, Min Length: 0, Avg Length: 1169.668, Median length: 489.000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8cc25a2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc_len =[]\n",
    "for doc in all_data:\n",
    "    doc_len.append(len(doc))\n",
    "\n",
    "# Document length statistics\n",
    "print \"Number of Samples: %i, Max Length: %i, Min Length: %i, Avg Length: %.3f, Median length: %.3f\" \\\n",
    "    %(len(doc_len), max(doc_len), min(doc_len), np.mean(doc_len), np.median(doc_len))\n",
    "\n",
    "plt.figure(figsize = (12, 8))\n",
    "n, bins, patches = plt.hist(doc_len, 300)\n",
    "plt.title(\"Document Length Distribution\")\n",
    "plt.xlabel(\"Word Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the doucment length distribution, it is very skewed. We decided to use 500 words (around median) as document length so the distance to reduce the impact of padding and cutting of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build Vocabulary using 20 newsgroup data set\n",
    "\n",
    "* Load all data from library => remove header/footer/quotes => clean string => create corpus => build vocabulary\n",
    "\n",
    "* To build corpus, we write to raw txt, then use NLTK to process the raw text to a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newsgroup_all.txt\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "f = open('./Data_Set/newsgroup_prep/newsgroup_all.txt', 'w') \n",
    "for doc in all_data:\n",
    "    # Clean up str\n",
    "    doc = utils.clean_str((doc).encode('utf-8'))\n",
    "    # remove stop words and do stemming optionaly\n",
    "    doc = utils.preprocess_stop_stem(doc, stop=True, sent=True, stem=False)\n",
    "    f.write(\"%s\\n\" %(doc))\n",
    "f.close()\n",
    "\n",
    "# RegEx or list of file names\n",
    "data_20newsgroup = os.path.join(PROJECT_DATA, 'newsgroup_prep/')\n",
    "\n",
    "corpus = PlaintextCorpusReader(data_20newsgroup, 'newsgroup_all.txt')\n",
    "\n",
    "for infile in sorted(corpus.fileids()):\n",
    "    print infile # The fileids of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 20000 words\n"
     ]
    }
   ],
   "source": [
    "V = 20000\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) \n",
    "                               for w in utils.flatten(corpus.sents())),\n",
    "                               size = V)\n",
    "print \"Vocabulary: %d words\" % vocab.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Data Setup\n",
    "\n",
    "We decided to use the following subset of 20 newsgroup data for our experiment grids. We choose smaller number of classes and subtset of data. We hope to visualized clustering to get more intuition.\n",
    "\n",
    "* 5 labeled class + 1 unknown class\n",
    "* 5 labeled class + 2 unknown classes\n",
    "* 5 labeled class + 3 unknown classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allow random selection of classes from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly Select Classes:  ['comp.windows.x', 'talk.politics.mideast', 'comp.sys.ibm.pc.hardware', 'sci.crypt', 'comp.os.ms-windows.misc', 'comp.sys.mac.hardware', 'sci.electronics']\n",
      "[0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Select training and test data based on the number of classes\n",
    "# Including randomization option\n",
    "import random\n",
    "from random import randint\n",
    "random.seed(8)\n",
    "\n",
    "num_class = 7\n",
    "randomize = True\n",
    "\n",
    "if randomize == True:\n",
    "    label_idxs = []\n",
    "    label_idxs = random.sample(range(1, 19), num_class)\n",
    "else:\n",
    "    label_idxs = range(num_class)\n",
    "\n",
    "select_classes = [label_list[i] for i in label_idxs]\n",
    "print \"Randomly Select Classes: \", select_classes\n",
    "\n",
    "newsgroups_all = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'),\n",
    "                                    categories=select_classes)\n",
    "\n",
    "all_data, all_labels = newsgroups_all.data, newsgroups_all.target\n",
    "print np.unique(all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define document length and create training, cross-validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6833 docs (2.08186e+07 tokens)\n",
      "Training set: 5466 docs (16656722 tokens)\n",
      "Test set: 1367 docs (4161876 tokens)\n",
      "Loaded 5466 docs (1.66567e+07 tokens)\n",
      "Training set: 4372 docs (13313358 tokens)\n",
      "Validation set: 1094 docs (3343364 tokens)\n"
     ]
    }
   ],
   "source": [
    "doc_length = 500\n",
    "\n",
    "# Preprocess data\n",
    "# Cleaning special characters\n",
    "# Cut or pad based on document length\n",
    "all_docs = utils.preprocess_doc(all_data, length = doc_length)\n",
    "\n",
    "# Split total data set to training and test set\n",
    "train_docs, train_labels, test_docs, test_labels = utils.get_train_test_docs(all_docs, \n",
    "                                                                             all_labels, \n",
    "                                                                             split = 0.8, \n",
    "                                                                             shuffle = True)\n",
    "\n",
    "# Further split training set into training and validation set\n",
    "train_docs, train_labels, validation_docs, validation_labels = utils.get_train_val_docs(train_docs, \n",
    "                                                                                         train_labels, \n",
    "                                                                                         split = 0.75, \n",
    "                                                                                         shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Training Docs shape: (4372, 500) should equal to (batch_size, doc_length)\n",
      "Input Training labels shape: (4372, 7) should equal to (batch_size, num_class)\n",
      "Input vaidation Docs shape: (1094, 500) should equal to (batch_size, doc_length)\n",
      "Input Validation labels shape: (1094, 7) should equal to (batch_size, num_class)\n",
      "Input Testing Docs shape: (1367, 500) should equal to (batch_size, doc_length)\n",
      "Input Testing labels shape: (1367, 7) should equal to (batch_size, num_class)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize documents and conver to ID\n",
    "# We tokenize each docs in the dataset and convert to vocab ID\n",
    "# matrix of batch_size x doc_length\n",
    "train_docs_ids = utils.docs_to_ids(train_docs, vocab)\n",
    "validation_docs_ids = utils.docs_to_ids(validation_docs, vocab)\n",
    "test_docs_ids = utils.docs_to_ids(test_docs, vocab)\n",
    "\n",
    "# Convert label to one-hot-code\n",
    "train_labels_oh = np.eye(num_class)[train_labels]\n",
    "validation_labels_oh = np.eye(num_class)[validation_labels]\n",
    "test_labels_oh = np.eye(num_class)[test_labels]\n",
    "\n",
    "print \"Input Training Docs shape:\", train_docs_ids.shape, \"should equal to (batch_size, doc_length)\"\n",
    "print \"Input Training labels shape:\", train_labels_oh.shape, \"should equal to (batch_size, num_class)\"\n",
    "print \"Input vaidation Docs shape:\", validation_docs_ids.shape, \"should equal to (batch_size, doc_length)\"\n",
    "print \"Input Validation labels shape:\", validation_labels_oh.shape, \"should equal to (batch_size, num_class)\"\n",
    "print \"Input Testing Docs shape:\", test_docs_ids.shape, \"should equal to (batch_size, doc_length)\"\n",
    "print \"Input Testing labels shape:\", test_labels_oh.shape, \"should equal to (batch_size, num_class)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prepare Data for the CNN\n",
    "\n",
    "### Option 1. Train word Embedding with CNN. \n",
    "\n",
    "#### Tokenize document and build input data for word2vec\n",
    "\n",
    "* Load data => remove header/footer/quotes => cleaned string => cut and pad => use vocabulary to tokenize\n",
    "\n",
    "* Need to enable randomly select a number of classes for training and test data\n",
    "\n",
    "* Instead of using Sk-learn's building selection of train or test data. Build our own train/test data based on %split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2 Load Google Pretrained Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_google_bin(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        print \"Google Word2vec Vocabulary Size:\", vocab_size\n",
    "        print \"Vector size:\", layer1_size\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        print \"Binary Length of word vector:\", binary_len\n",
    "        for line in xrange(vocab_size):\n",
    "            word = []\n",
    "            while True: # Read 1 char a time\n",
    "                ch = f.read(1) \n",
    "                if ch == ' ': # If it is a space, a word is read, we join then to read its vector\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n': # If it is not \\n, grouping character\n",
    "                    word.append(ch) \n",
    "            if word in vocab.wordset: # If a word in the 20 newsgroup vocab, get its vector\n",
    "                word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    f.close()\n",
    "    return word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Word2vec Vocabulary Size: 3000000\n",
      "Vector size: 300\n",
      "Binary Length of word vector: 1200\n"
     ]
    }
   ],
   "source": [
    "google_word2vec = load_google_bin('./google_word2vec/GoogleNews-vectors-negative300.bin', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of vocabulary in 20newsgroup: 20000\n",
      "Total matched vocabulary from google word2vec: 16555\n",
      "--- Print a sample of google_word2vec vocabulary ---\n",
      "Word: raining \t\t Vector: [ 0.02331543  0.05004883 -0.00059891] ...\n",
      "Word: writings \t\t Vector: [ 0.18945312  0.2109375   0.20507812] ...\n",
      "Word: divinely \t\t Vector: [-0.02783203 -0.40820312 -0.01037598] ...\n",
      "Word: foul \t\t Vector: [ 0.18847656 -0.28710938  0.33007812] ...\n",
      "Word: four \t\t Vector: [ 0.0859375  -0.07275391  0.01672363] ...\n",
      "Word: gag \t\t Vector: [ 0.14648438 -0.08203125 -0.00897217] ...\n",
      "Word: prefix \t\t Vector: [ 0.34570312  0.1640625   0.11425781] ...\n",
      "Word: woods \t\t Vector: [ 0.11328125 -0.01165771 -0.20800781] ...\n",
      "Word: verses \t\t Vector: [ 0.28710938  0.15820312  0.23828125] ...\n",
      "Word: hanging \t\t Vector: [ 0.08984375  0.13769531 -0.14941406] ...\n",
      "Word: woody \t\t Vector: [ 0.08251953  0.44140625  0.07421875] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Total Number of vocabulary in 20newsgroup:\", vocab.size\n",
    "print \"Total matched vocabulary from google word2vec:\", len(google_word2vec.keys())\n",
    "print \"--- Print a sample of google_word2vec vocabulary ---\"\n",
    "i = 0\n",
    "for k, v in google_word2vec.iteritems():\n",
    "    if i <= 10:\n",
    "        print \"Word: %s \\t\\t Vector: %s ...\" %(k, v[:3])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example vocabulary of the 20 newsgroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'\\b([A-Za-z]+)\\b'\n",
    "count_w = []\n",
    "count_s = []\n",
    "for word in vocab.wordset:    \n",
    "    if re.search(regex, str(word)):\n",
    "        count_w.append(str(word))\n",
    "    else:\n",
    "        count_s.append(str(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of full letter words 18724\n",
      "Number of mixed ascii words 1276\n"
     ]
    }
   ],
   "source": [
    "print \"Number of full letter words\", len(count_w)\n",
    "print \"Number of mixed ascii words\", len(count_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take care of unknown words from 20newsgroup that does not exist in google word2vec\n",
    "\n",
    "* 0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unknown_words(google_word2vec, vocab, k=300):\n",
    "    for word in vocab.wordset:\n",
    "        if word not in google_word2vec:\n",
    "            google_word2vec[word] = np.random.uniform(-0.25,0.25,k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_unknown_words(google_word2vec, vocab, k=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of vocabulary in 20newsgroup: 20000\n",
      "Total matched vocabulary from google word2vec: 20000\n",
      "Pre-trained word2vec size (20000, 300)\n"
     ]
    }
   ],
   "source": [
    "print \"Total Number of vocabulary in 20newsgroup:\", vocab.size\n",
    "print \"Total matched vocabulary from google word2vec:\", len(google_word2vec.keys())\n",
    "pt_word2vec = np.array(google_word2vec.values())\n",
    "print \"Pre-trained word2vec size\", pt_word2vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Illustrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cnn.png\" width = \"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DOC Paper Terminology and Notes:\n",
    "\n",
    "* Embedding Layer:\n",
    "\n",
    "    * batch_size: N\n",
    "    * number of class: M\n",
    "    * A document matrix has size of $s \\times d$\n",
    "        * word embedding dimension: d => 300\n",
    "        * doc length: s => 2000\n",
    "    \n",
    "    \n",
    "* CNN Layer\n",
    "\n",
    "    * Region size: h => [3, 4, 5]\n",
    "    * Width of the filter: d => 300\n",
    "    * Number of filters per region: f => 150\n",
    "    * CNN out o: k => f x h = 150 x 3 = 450\n",
    "    \n",
    "\n",
    "* Output Layer\n",
    "    * W: $M \\times k$\n",
    "    * y: $M$\n",
    "\n",
    "Note: The DOC Paper used a 2 fully connected output layer before softmax which is different from common approach\n",
    "\n",
    "$$out = Wo + b$$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Paramters\n",
    "# doc_length = 100 => to be aligned with DOC paper?\n",
    "# num_class = 3 => to be adjusted based on stratified CV strategy from the DOC paper\n",
    "# vocab.size => If we don't use google word2vec, it will be vocab.size from newsgroup data alone\n",
    "# embedding_size => to be aligned with DOC paper\n",
    "# filter_size = [3, 4] => to be aligned with DOC paper\n",
    "# num_filters = 100 => to be aligned w\n",
    "\n",
    "doc_length = doc_length # s\n",
    "num_classes = num_class # M\n",
    "vocab_size = vocab.size\n",
    "embedding_size = 300 # d\n",
    "embedding_train = False # if True, we train word embedding, if False we use pretrained word2vec\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 150 # f\n",
    "l2_reg_lambda = 0.0\n",
    "dropout_prob = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for input, output and dropout\n",
    "# x_: Document, Size: (batch, document_length) word in indice\n",
    "# y_: Classes, Size: (batch, num_of_classes)\n",
    "# dropout_keep_prob: Dropout regularization parameter\n",
    "x_ = tf.placeholder(tf.int32, [None, doc_length], name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes], name=\"y\")\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "# Keeping track of l2 regularization loss (optional)\n",
    "l2_loss = tf.constant(0.0)\n",
    "\n",
    "# Embedding layer (Train embedding layer)\n",
    "# Need different implementation if use google pretrained word2vec\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "    # The vocab to vector table for lookup (to be trained or pre-trained)\n",
    "    if embedding_train:\n",
    "        C_ = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"C\")\n",
    "    else:\n",
    "        C_ = tf.placeholder(tf.float32, [vocab_size, embedding_size], name=\"C\")\n",
    "\n",
    "    # Embedding output needs to be in size: (batch, doc_length, embedding_size, 1)\n",
    "    # Lookup gives (batch, doc_length, embedding_size)\n",
    "    # Therefore, we need to expand the dimension to 4D to work with conv2d\n",
    "    embedded_out = tf.expand_dims(tf.nn.embedding_lookup(C_, x_), -1)\n",
    "\n",
    "# Create a convolution + maxpool layer for each filter size\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "\n",
    "        # Convolution Layer\n",
    "        # input shape: (batch, height(doc length, width(embedding size), channels(1) )\n",
    "        # filter shape: (filter_height, filter width(same as embedding size), in_channel, out_channels)\n",
    "        # in_channel = 1 for our data\n",
    "        # out_channel = num_filters\n",
    "        filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "        # To experiment with normal distribution\n",
    "        W_ = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "        b_ = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "\n",
    "        # \"VALID\" padding means no padding at edge\n",
    "        # Return shape (batch, height(doc length, width(embedding size), 1)\n",
    "        conv_ = tf.nn.conv2d(embedded_out, W_, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
    "\n",
    "        # Apply nonlinearity using Relu (train fasster than tanh)\n",
    "        # Return shape (batch, height(doc length, 1, 1)\n",
    "        h_ = tf.nn.relu(tf.nn.bias_add(conv_, b_), name=\"relu\")\n",
    "\n",
    "        # Maxpooling over the outputs\n",
    "        # ksize is window for pooling, we took 1 value for width direction\n",
    "        # For height, apply to each convolution steps to stripe the whole input matrix.\n",
    "        # Return shape (1, doc_length-filter_size+1, 1, 1)\n",
    "        pooled = tf.nn.max_pool(h_, \n",
    "                                ksize=[1, doc_length - filter_size + 1, 1, 1],\n",
    "                                strides=[1, 1, 1, 1], \n",
    "                                padding='VALID', \n",
    "                                name=\"pool\")\n",
    "        pooled_outputs.append(pooled)\n",
    "\n",
    "# Combine all the pooled features\n",
    "# find the total number of filters = num_of_filters * num_of_region\n",
    "# If we use [2, 3, 4] and 2 filter per region, we have 3 * 2 = 6 filters\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "\n",
    "# combine pooling output to feature vectors\n",
    "# h_pool_flat in shape of (batch_size, ? , num_filters_total)\n",
    "h_pool = tf.concat(pooled_outputs, 3)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "# Add dropout\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "\n",
    "# Output Layer: Softmax\n",
    "# Final (unnormalized) scores and predictions\n",
    "# Do we need to normalize?\n",
    "with tf.name_scope(\"Output_layer\"):\n",
    "    Z_ = tf.Variable(tf.random_uniform([num_filters_total, num_class], -1.0, 1.0), name = \"Z\")\n",
    "    b_output_ = tf.Variable(tf.constant(0.1, shape=[num_class]), name=\"b_output\")\n",
    "    logits_ = tf.add(tf.matmul(h_drop, Z_), b_output_, name=\"logits\")\n",
    "\n",
    "    # L2 loss\n",
    "    l2_loss += tf.nn.l2_loss(Z_)\n",
    "    l2_loss += tf.nn.l2_loss(b_)\n",
    "\n",
    "    #scores = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\n",
    "    predictions_ = tf.argmax(logits_, 1, name=\"predictions\")\n",
    "\n",
    "# Calculate mean cross-entropy loss\n",
    "with tf.name_scope(\"cost_function\"):\n",
    "    per_example_losses_ = tf.nn.softmax_cross_entropy_with_logits(logits=logits_, \n",
    "                                                                 labels=y_,\n",
    "                                                                 name=\"per_example_loss\")\n",
    "    loss_ = tf.reduce_mean(per_example_losses_) + l2_reg_lambda * l2_loss\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions_ = tf.equal(predictions_, tf.argmax(y_, 1))\n",
    "    accuracy_ = tf.reduce_mean(tf.cast(correct_predictions_, \"float\"), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"Training\"):\n",
    "    alpha_ = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    optimizer_ = tf.train.AdagradOptimizer(alpha_)\n",
    "    #optimizer_ = tf.train.AdamOptimizer(alpha_)\n",
    "    train_step_ = optimizer_.minimize(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for training\n",
    "def display_params():\n",
    "    print \"doc_length: {:<10d}\".format(doc_length) # s\n",
    "    print \"num_classes: {:<10d}\".format(num_class) # M\n",
    "    print \"vocabulary size: {:<10d}\".format(vocab.size)\n",
    "    print \"embedding size: {:<10d}\".format(embedding_size) # d\n",
    "    print \"Train embedding? \", embedding_train\n",
    "    print \"filter size: {:<10s}\".format(filter_sizes)\n",
    "    print \"number of filters: {:<10d}\".format(num_filters)\n",
    "    print \"L2 Regularization: {:<10.2f}\".format(l2_reg_lambda)\n",
    "    print \"Keep drop out prob: {:<10.2f}\".format(dropout_prob)\n",
    "    \n",
    "def train_batch(session, batch, alpha, train_embedding):\n",
    "    # Feed last column as targets\n",
    "    if train_embedding:\n",
    "        feed_dict = {x_:train_docs_ids,\n",
    "                     y_:train_labels_oh,\n",
    "                     dropout_keep_prob:dropout_prob, # No dropout\n",
    "                     alpha_:alpha}\n",
    "    else:\n",
    "        feed_dict = {x_:train_docs_ids,\n",
    "                     y_:train_labels_oh,\n",
    "                     C_:pt_word2vec, # use google word2vec\n",
    "                     dropout_keep_prob:dropout_prob, # No dropout\n",
    "                     alpha_:alpha}\n",
    "    c, a, pred, _ = session.run([loss_, accuracy_, predictions_, train_step_],\n",
    "                       feed_dict=feed_dict)\n",
    "    return c, a, pred\n",
    "\n",
    "def validation_batch(session):\n",
    "    feed_dict = {x_:test_docs_ids,\n",
    "                 y_:test_labels_oh,\n",
    "                 C_:pt_word2vec,\n",
    "                 dropout_keep_prob:dropout_prob}\n",
    "    a, pred = session.run([accuracy_, predictions_, ], feed_dict=feed_dict)\n",
    "    return a, pred\n",
    "\n",
    "def batch_generator(data, batch_size):\n",
    "    \"\"\"Generate minibatches from data.\"\"\"\n",
    "    for i in xrange(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]\n",
    "        \n",
    "def predict_batch_te(session):\n",
    "    feed_dict = {x_:test_docs_ids,\n",
    "                 y_:test_labels_oh,\n",
    "                 dropout_keep_prob:dropout_prob}\n",
    "    a, pred = session.run([accuracy_, predictions_], feed_dict=feed_dict)\n",
    "    return a, pred\n",
    "\n",
    "def predict_batch_w2v(session):\n",
    "    feed_dict = {x_:test_docs_ids,\n",
    "                 y_:test_labels_oh,\n",
    "                 C_:pt_word2vec,\n",
    "                 dropout_keep_prob:dropout_prob}\n",
    "    a, pred = session.run([accuracy_, predictions_, ], feed_dict=feed_dict)\n",
    "    return a, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def run_epochs(num_epoches, learning_rate, trained_filename, batch_size = 100,\n",
    "               min_rate = 0.1, print_freq = 10, seed = 42, train_embedding = True):\n",
    "# One epoch = one pass through the training data\n",
    "    num_epochs = num_epoches\n",
    "    batch_size = batch_size\n",
    "    alpha = learning_rate  # learning rate\n",
    "    min_alpha = min_rate\n",
    "    alpha_delta = (alpha - min_alpha) / num_epochs\n",
    "    print_every = print_freq\n",
    "\n",
    "    # Initializer step\n",
    "    init_ = tf.global_variables_initializer()\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # For plotting\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(init_)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        epoch_cost = 0.0\n",
    "        total_batches = 0\n",
    "        print \"\"\n",
    "        print \"----------- Training ------------\"\n",
    "        for i, batch in enumerate(batch_generator(train_docs_ids, batch_size)):\n",
    "            if (i % print_every == 0):\n",
    "                print \"[epoch %d] seen %d minibatches\" % (epoch, i)\n",
    "\n",
    "            cost, accuracy, pred = train_batch(session, batch, alpha, train_embedding)\n",
    "            epoch_cost += cost\n",
    "            total_batches = i + 1\n",
    "\n",
    "        avg_cost = epoch_cost / total_batches\n",
    "        alpha = alpha - alpha_delta\n",
    "\n",
    "        print \"[epoch %d] Completed %d minibatches in %s\" % (epoch, i, utils.pretty_timedelta(since=t0_epoch))\n",
    "        print \"[epoch %d] Average cost: %.03f\" % (epoch, avg_cost,)\n",
    "        print \"[epoch %d] Accuracy %.03f\" %(epoch, accuracy)\n",
    "        print \"[epoch %d] Training Classificaiton Report\\n\" %(epoch)\n",
    "        print classification_report(train_labels, pred)\n",
    "        print \"Training Vector Accuracy: \", accuracy_score(train_labels, pred)\n",
    "        train_accuracy.append(accuracy)\n",
    "        \n",
    "        print \"\"\n",
    "        print \"---------- Test ----------\"\n",
    "        if train_embedding:\n",
    "            accuracy, pred = predict_batch_te(session)\n",
    "        else:\n",
    "            accuracy, pred = predict_batch_w2v(session)\n",
    "        print \"[epoch %d] Test Accuracy is %.03f\" %(epoch, accuracy)\n",
    "        print \"[epoch %d] Test Classificaiton Report\\n\" %(epoch)\n",
    "        print classification_report(test_labels, pred)\n",
    "        print \"Test Vector Accuracy:\", accuracy_score(test_labels, pred)\n",
    "        test_accuracy.append(accuracy)\n",
    "        \n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(session, \"/tmp/model\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "    return train_accuracy, test_accuracy, session\n",
    "        \n",
    "def plot_learning(num_epochs, train_accuracy, test_accuracy):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.plot(range(0, num_epochs), train_accuracy, '.-', label = \"Training accuracy\")\n",
    "    plt.plot(range(0, num_epochs), test_accuracy, '.-', label = \"Test accuracy\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Decision 1. Google word2vec vs. Training word embedding on the fly using \n",
    "\n",
    "* 5 classes\n",
    "* no regularization\n",
    "\n",
    "#### Google Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_length: 500       \n",
      "num_classes: 8         \n",
      "vocabulary size: 20000     \n",
      "embedding size: 300       \n",
      "Train embedding?  False\n",
      "filter size: [3, 4, 5] \n",
      "number of filters: 150       \n",
      "L2 Regularization: 0.00      \n",
      "Keep drop out prob: 1.00      \n"
     ]
    }
   ],
   "source": [
    "display_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Training ------------\n",
      "[epoch 1] seen 0 minibatches\n",
      "[epoch 1] seen 10 minibatches\n",
      "[epoch 1] seen 20 minibatches\n",
      "[epoch 1] seen 30 minibatches\n",
      "[epoch 1] Completed 31 minibatches in 0:21:28\n",
      "[epoch 1] Average cost: 41.010\n",
      "[epoch 1] Accuracy 0.761\n",
      "[epoch 1] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.87      0.63       646\n",
      "          1       0.91      0.59      0.71       615\n",
      "          2       0.92      0.83      0.87       645\n",
      "          3       0.92      0.72      0.81       641\n",
      "          4       0.93      0.79      0.85       579\n",
      "\n",
      "avg / total       0.83      0.76      0.77      3126\n",
      "\n",
      "Training Vector Accuracy:  0.761356365963\n",
      "\n",
      "---------- Test ----------\n",
      "[epoch 1] Test Accuracy is 0.483\n",
      "[epoch 1] Test Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.30      0.62      0.41       176\n",
      "          1       0.51      0.33      0.40       202\n",
      "          2       0.66      0.67      0.66       180\n",
      "          3       0.48      0.37      0.42       208\n",
      "          4       0.72      0.46      0.56       212\n",
      "\n",
      "avg / total       0.54      0.48      0.49       978\n",
      "\n",
      "Test Vector Accuracy: 0.482617586912\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 2] seen 0 minibatches\n",
      "[epoch 2] seen 10 minibatches\n",
      "[epoch 2] seen 20 minibatches\n",
      "[epoch 2] seen 30 minibatches\n",
      "[epoch 2] Completed 31 minibatches in 0:20:55\n",
      "[epoch 2] Average cost: 0.522\n",
      "[epoch 2] Accuracy 0.898\n",
      "[epoch 2] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.84      0.88       646\n",
      "          1       0.76      0.92      0.83       615\n",
      "          2       0.96      0.94      0.95       645\n",
      "          3       0.97      0.88      0.92       641\n",
      "          4       0.93      0.91      0.92       579\n",
      "\n",
      "avg / total       0.91      0.90      0.90      3126\n",
      "\n",
      "Training Vector Accuracy:  0.89795265515\n",
      "\n",
      "---------- Test ----------\n",
      "[epoch 2] Test Accuracy is 0.544\n",
      "[epoch 2] Test Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.56      0.50       176\n",
      "          1       0.43      0.50      0.47       202\n",
      "          2       0.67      0.67      0.67       180\n",
      "          3       0.53      0.47      0.50       208\n",
      "          4       0.69      0.54      0.61       212\n",
      "\n",
      "avg / total       0.56      0.54      0.55       978\n",
      "\n",
      "Test Vector Accuracy: 0.543967280164\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 3] seen 0 minibatches\n",
      "[epoch 3] seen 10 minibatches\n",
      "[epoch 3] seen 20 minibatches\n",
      "[epoch 3] seen 30 minibatches\n",
      "[epoch 3] Completed 31 minibatches in 0:19:47\n",
      "[epoch 3] Average cost: 0.318\n",
      "[epoch 3] Accuracy 0.928\n",
      "[epoch 3] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.88      0.92       646\n",
      "          1       0.80      0.96      0.87       615\n",
      "          2       0.98      0.96      0.97       645\n",
      "          3       0.99      0.91      0.95       641\n",
      "          4       0.95      0.92      0.94       579\n",
      "\n",
      "avg / total       0.94      0.93      0.93      3126\n",
      "\n",
      "Training Vector Accuracy:  0.928342930262\n",
      "\n",
      "---------- Test ----------\n",
      "[epoch 3] Test Accuracy is 0.557\n",
      "[epoch 3] Test Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.55      0.51       176\n",
      "          1       0.45      0.51      0.48       202\n",
      "          2       0.68      0.68      0.68       180\n",
      "          3       0.54      0.50      0.52       208\n",
      "          4       0.69      0.56      0.62       212\n",
      "\n",
      "avg / total       0.57      0.56      0.56       978\n",
      "\n",
      "Test Vector Accuracy: 0.557259713701\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 4] seen 0 minibatches\n",
      "[epoch 4] seen 10 minibatches\n",
      "[epoch 4] seen 20 minibatches\n",
      "[epoch 4] seen 30 minibatches\n",
      "[epoch 4] Completed 31 minibatches in 0:19:53\n",
      "[epoch 4] Average cost: 0.241\n",
      "[epoch 4] Accuracy 0.940\n",
      "[epoch 4] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.90      0.94       646\n",
      "          1       0.82      0.98      0.89       615\n",
      "          2       0.98      0.96      0.97       645\n",
      "          3       0.99      0.93      0.96       641\n",
      "          4       0.96      0.93      0.95       579\n",
      "\n",
      "avg / total       0.95      0.94      0.94      3126\n",
      "\n",
      "Training Vector Accuracy:  0.940499040307\n",
      "\n",
      "---------- Test ----------\n",
      "[epoch 4] Test Accuracy is 0.570\n",
      "[epoch 4] Test Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.58      0.53       176\n",
      "          1       0.46      0.51      0.48       202\n",
      "          2       0.68      0.67      0.67       180\n",
      "          3       0.56      0.52      0.54       208\n",
      "          4       0.71      0.58      0.64       212\n",
      "\n",
      "avg / total       0.58      0.57      0.57       978\n",
      "\n",
      "Test Vector Accuracy: 0.569529652352\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 5] seen 0 minibatches\n",
      "[epoch 5] seen 10 minibatches\n",
      "[epoch 5] seen 20 minibatches\n",
      "[epoch 5] seen 30 minibatches\n",
      "[epoch 5] Completed 31 minibatches in 0:19:37\n",
      "[epoch 5] Average cost: 0.204\n",
      "[epoch 5] Accuracy 0.946\n",
      "[epoch 5] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.91      0.94       646\n",
      "          1       0.83      0.99      0.90       615\n",
      "          2       0.98      0.97      0.98       645\n",
      "          3       0.99      0.93      0.96       641\n",
      "          4       0.97      0.93      0.95       579\n",
      "\n",
      "avg / total       0.95      0.95      0.95      3126\n",
      "\n",
      "Training Vector Accuracy:  0.945937300064\n",
      "\n",
      "---------- Test ----------\n",
      "[epoch 5] Test Accuracy is 0.583\n",
      "[epoch 5] Test Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.61      0.55       176\n",
      "          1       0.49      0.53      0.51       202\n",
      "          2       0.69      0.67      0.68       180\n",
      "          3       0.57      0.53      0.55       208\n",
      "          4       0.71      0.58      0.64       212\n",
      "\n",
      "avg / total       0.59      0.58      0.59       978\n",
      "\n",
      "Test Vector Accuracy: 0.58282208589\n"
     ]
    }
   ],
   "source": [
    "train_accur, test_accur, sess = run_epochs(num_epoches = 5,\n",
    "                                           learning_rate = 0.5, \n",
    "                                           batch_size = 100,\n",
    "                                           min_rate = 0.1, \n",
    "                                           print_freq = 10, \n",
    "                                           seed = 42, \n",
    "                                           train_embedding = embedding_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train word embedding on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_length: 500       \n",
      "num_classes: 5         \n",
      "vocabulary size: 20000     \n",
      "embedding size: 300       \n",
      "Train embedding?  True\n",
      "filter size: [3, 4, 5] \n",
      "number of filters: 150       \n",
      "L2 Regularization: 0.00      \n",
      "Keep drop out prob: 1.00      \n"
     ]
    }
   ],
   "source": [
    "display_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Training ------------\n",
      "[epoch 1] seen 0 minibatches\n",
      "[epoch 1] seen 10 minibatches\n",
      "[epoch 1] seen 20 minibatches\n",
      "[epoch 1] seen 30 minibatches\n",
      "[epoch 1] Completed 31 minibatches in 0:22:15\n",
      "[epoch 1] Average cost: 380.543\n",
      "[epoch 1] Accuracy 0.900\n",
      "[epoch 1] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.90      0.81       646\n",
      "          1       0.92      0.91      0.91       615\n",
      "          2       0.99      0.90      0.94       645\n",
      "          3       0.98      0.90      0.94       641\n",
      "          4       0.93      0.88      0.90       579\n",
      "\n",
      "avg / total       0.91      0.90      0.90      3126\n",
      "\n",
      "Training Vector Accuracy:  0.899552143314\n",
      "\n",
      "---------- Test ----------\n",
      "[epoch 1] Test Accuracy is 0.415\n",
      "[epoch 1] Test Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.41      0.38       176\n",
      "          1       0.52      0.36      0.43       202\n",
      "          2       0.38      0.60      0.46       180\n",
      "          3       0.45      0.51      0.48       208\n",
      "          4       0.41      0.21      0.28       212\n",
      "\n",
      "avg / total       0.43      0.42      0.41       978\n",
      "\n",
      "Test Vector Accuracy: 0.415132924335\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 2] seen 0 minibatches\n",
      "[epoch 2] seen 10 minibatches\n",
      "[epoch 2] seen 20 minibatches\n",
      "[epoch 2] seen 30 minibatches\n",
      "[epoch 2] Completed 31 minibatches in 0:21:18\n",
      "[epoch 2] Average cost: 0.234\n",
      "[epoch 2] Accuracy 0.921\n",
      "[epoch 2] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.95      0.85       646\n",
      "          1       0.95      0.94      0.95       615\n",
      "          2       0.99      0.90      0.95       645\n",
      "          3       0.99      0.91      0.95       641\n",
      "          4       0.97      0.90      0.93       579\n",
      "\n",
      "avg / total       0.93      0.92      0.92      3126\n",
      "\n",
      "Training Vector Accuracy:  0.921305182342\n",
      "\n",
      "---------- Test ----------\n",
      "[epoch 2] Test Accuracy is 0.425\n",
      "[epoch 2] Test Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.37      0.47      0.41       176\n",
      "          1       0.53      0.38      0.44       202\n",
      "          2       0.37      0.59      0.46       180\n",
      "          3       0.46      0.51      0.49       208\n",
      "          4       0.45      0.21      0.29       212\n",
      "\n",
      "avg / total       0.44      0.43      0.42       978\n",
      "\n",
      "Test Vector Accuracy: 0.425357873211\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 3] seen 0 minibatches\n",
      "[epoch 3] seen 10 minibatches\n",
      "[epoch 3] seen 20 minibatches\n",
      "[epoch 3] seen 30 minibatches\n",
      "[epoch 3] Completed 31 minibatches in 0:20:35\n",
      "[epoch 3] Average cost: 0.199\n",
      "[epoch 3] Accuracy 0.934\n",
      "[epoch 3] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.99      0.87       646\n",
      "          1       0.98      0.95      0.97       615\n",
      "          2       1.00      0.91      0.95       645\n",
      "          3       0.99      0.92      0.95       641\n",
      "          4       0.98      0.91      0.94       579\n",
      "\n",
      "avg / total       0.95      0.93      0.94      3126\n",
      "\n",
      "Training Vector Accuracy:  0.934420985285\n",
      "\n",
      "---------- Test ----------\n",
      "[epoch 3] Test Accuracy is 0.428\n",
      "[epoch 3] Test Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.37      0.46      0.41       176\n",
      "          1       0.54      0.40      0.46       202\n",
      "          2       0.38      0.59      0.46       180\n",
      "          3       0.46      0.51      0.49       208\n",
      "          4       0.44      0.21      0.28       212\n",
      "\n",
      "avg / total       0.44      0.43      0.42       978\n",
      "\n",
      "Test Vector Accuracy: 0.428425357873\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 4] seen 0 minibatches\n",
      "[epoch 4] seen 10 minibatches\n",
      "[epoch 4] seen 20 minibatches\n",
      "[epoch 4] seen 30 minibatches\n",
      "[epoch 4] Completed 31 minibatches in 0:20:27\n",
      "[epoch 4] Average cost: 0.184\n",
      "[epoch 4] Accuracy 0.937\n",
      "[epoch 4] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.99      0.88       646\n",
      "          1       0.98      0.96      0.97       615\n",
      "          2       1.00      0.91      0.95       645\n",
      "          3       0.99      0.92      0.95       641\n",
      "          4       0.99      0.91      0.95       579\n",
      "\n",
      "avg / total       0.95      0.94      0.94      3126\n",
      "\n",
      "Training Vector Accuracy:  0.936980166347\n",
      "\n",
      "---------- Test ----------\n",
      "[epoch 4] Test Accuracy is 0.431\n",
      "[epoch 4] Test Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.38      0.48      0.42       176\n",
      "          1       0.56      0.41      0.47       202\n",
      "          2       0.38      0.58      0.46       180\n",
      "          3       0.46      0.50      0.48       208\n",
      "          4       0.44      0.21      0.28       212\n",
      "\n",
      "avg / total       0.45      0.43      0.42       978\n",
      "\n",
      "Test Vector Accuracy: 0.431492842536\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 5] seen 0 minibatches\n",
      "[epoch 5] seen 10 minibatches\n",
      "[epoch 5] seen 20 minibatches\n",
      "[epoch 5] seen 30 minibatches\n",
      "[epoch 5] Completed 31 minibatches in 0:19:57\n",
      "[epoch 5] Average cost: 0.175\n",
      "[epoch 5] Accuracy 0.939\n",
      "[epoch 5] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.99      0.88       646\n",
      "          1       0.98      0.96      0.97       615\n",
      "          2       1.00      0.91      0.95       645\n",
      "          3       1.00      0.92      0.96       641\n",
      "          4       0.99      0.91      0.95       579\n",
      "\n",
      "avg / total       0.95      0.94      0.94      3126\n",
      "\n",
      "Training Vector Accuracy:  0.938899552143\n",
      "\n",
      "---------- Test ----------\n",
      "[epoch 5] Test Accuracy is 0.431\n",
      "[epoch 5] Test Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.37      0.49      0.42       176\n",
      "          1       0.57      0.41      0.47       202\n",
      "          2       0.38      0.58      0.46       180\n",
      "          3       0.46      0.50      0.48       208\n",
      "          4       0.45      0.21      0.28       212\n",
      "\n",
      "avg / total       0.45      0.43      0.42       978\n",
      "\n",
      "Test Vector Accuracy: 0.431492842536\n"
     ]
    }
   ],
   "source": [
    "train_accur, test_accur = run_epochs(num_epoches = 5,\n",
    "                                     learning_rate = 0.5, \n",
    "                                     batch_size = 100,\n",
    "                                     min_rate = 0.1, \n",
    "                                     print_freq = 10, \n",
    "                                     seed = 42, \n",
    "                                     train_embedding = embedding_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Conclusion: Google word2vec out-perform training word-embedding on the fly. We use google word2vec for our word embedding layer **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Open Classificaiton Methods (1-vs-Rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 1-vs-Rest Layer of DOC\n",
    "\n",
    "    * M (number of class) sigmoid function, N (batch_size)\n",
    "    * Objective function for training is $$loss = \\sum_{i=1}^M \\sum_{i=1}^N y_n log(p) + (1 - y_n)log(1 - p(y))$$ is the summation of all log loss (cross-entropy) on the training data.\n",
    "    * At prediction, reject if all predicted probability is less than their threshold t_i, otherwise $argmax(Sigmoid(d))$\n",
    "    * The theshold is determined by using outlier detection. (***We can use a fixed number such as 0.95 to validate our model implementation***)\n",
    "    * Approach:\n",
    "        1. Remove some classes from the training data\n",
    "        2. Training the data\n",
    "        3. Add those classes back to the test data\n",
    "\n",
    "* Clustering Approach\n",
    "    * KNN\n",
    "    * Gausian Mix Model\n",
    "    * Infinite Dirichlet process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Use Google word2vec for CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Cross Validation Run Epochs **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs_cv(num_epoches, learning_rate, batch_size = 100, \n",
    "                  min_rate = 0.1, print_freq = 10, seed = 42, train_embedding = True):\n",
    "# One epoch = one pass through the training data\n",
    "    num_epochs = num_epoches\n",
    "    batch_size = batch_size\n",
    "    alpha = learning_rate  # learning rate\n",
    "    min_alpha = min_rate\n",
    "    alpha_delta = (alpha - min_alpha) / num_epochs\n",
    "    print_every = print_freq\n",
    "\n",
    "    # Initializer step\n",
    "    init_ = tf.global_variables_initializer()\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # For plotting\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "\n",
    "    session = tf.Session()\n",
    "    session.run(init_)\n",
    "\n",
    "    t0 = time.time()\n",
    "    for epoch in xrange(1,num_epochs+1):\n",
    "        t0_epoch = time.time()\n",
    "        epoch_cost = 0.0\n",
    "        total_batches = 0\n",
    "        print \"\"\n",
    "        print \"----------- Training ------------\"\n",
    "        for i, batch in enumerate(batch_generator(train_docs_ids, batch_size)):\n",
    "            if (i % print_every == 0):\n",
    "                print \"[epoch %d] seen %d minibatches\" % (epoch, i)\n",
    "\n",
    "            cost, accuracy, pred = train_batch(session, batch, alpha, train_embedding)\n",
    "            epoch_cost += cost\n",
    "            total_batches = i + 1\n",
    "\n",
    "        avg_cost = epoch_cost / total_batches\n",
    "        alpha = alpha - alpha_delta\n",
    "\n",
    "        print \"[epoch %d] Completed %d minibatches in %s\" % (epoch, i, utils.pretty_timedelta(since=t0_epoch))\n",
    "        print \"[epoch %d] Average cost: %.03f\" % (epoch, avg_cost,)\n",
    "        print \"[epoch %d] Accuracy %.03f\" %(epoch, accuracy)\n",
    "        print \"[epoch %d] Training Classificaiton Report\\n\" %(epoch)\n",
    "        print classification_report(train_labels, pred)\n",
    "        print \"Training Vector Accuracy: \", accuracy_score(train_labels, pred)\n",
    "        train_accuracy.append(accuracy)\n",
    "        \n",
    "        print \"\"\n",
    "        print \"---------- Validation ----------\"\n",
    "        accuracy, pred = validation_batch(session)\n",
    "        print \"[epoch %d] Validation Accuracy is %.03f\" %(epoch, accuracy)\n",
    "        print \"[epoch %d] Validation Classificaiton Report\\n\" %(epoch)\n",
    "        print classification_report(test_labels, pred)\n",
    "        print \"Test Vector Accuracy:\", accuracy_score(test_labels, pred)\n",
    "        test_accuracy.append(accuracy)        \n",
    "\n",
    "    # Save the variables to disk.\n",
    "    save_path = saver.save(session, \"/tmp/model\")\n",
    "    print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "    return train_accuracy, test_accuracy, session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1 (5 labeled + 1 unseen)\n",
    "\n",
    "### Remove 1 class and cross validate train data for hyper parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Params **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_length: 500       \n",
      "num_classes: 6         \n",
      "vocabulary size: 20000     \n",
      "embedding size: 300       \n",
      "Train embedding?  False\n",
      "filter size: [3, 4, 5] \n",
      "number of filters: 150       \n",
      "L2 Regularization: 0.50      \n",
      "Keep drop out prob: 1.00      \n"
     ]
    }
   ],
   "source": [
    "doc_length = doc_length # s\n",
    "num_classes = num_class # M\n",
    "vocab_size = vocab.size\n",
    "embedding_size = 300 # d\n",
    "embedding_train = False # We use pretrained word2vec\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 150\n",
    "l2_reg_lambda = 0.5\n",
    "dropout_prob = 1.0\n",
    "display_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Training ------------\n",
      "[epoch 1] seen 0 minibatches\n",
      "[epoch 1] seen 10 minibatches\n",
      "[epoch 1] seen 20 minibatches\n",
      "[epoch 1] seen 30 minibatches\n",
      "[epoch 1] Completed 37 minibatches in 0:21:11\n",
      "[epoch 1] Average cost: 54.312\n",
      "[epoch 1] Accuracy 0.707\n",
      "[epoch 1] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.63      0.72       630\n",
      "          1       0.40      0.78      0.53       631\n",
      "          2       0.85      0.61      0.71       629\n",
      "          3       0.83      0.75      0.79       619\n",
      "          4       0.84      0.74      0.79       620\n",
      "          5       0.92      0.74      0.82       614\n",
      "\n",
      "avg / total       0.78      0.71      0.72      3743\n",
      "\n",
      "Training Vector Accuracy:  0.706919583222\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 1] Validation Accuracy is 0.393\n",
      "[epoch 1] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.52      0.33      0.40       202\n",
      "          1       0.26      0.54      0.35       191\n",
      "          2       0.38      0.24      0.29       188\n",
      "          3       0.53      0.45      0.49       205\n",
      "          4       0.40      0.31      0.35       202\n",
      "          5       0.46      0.48      0.47       182\n",
      "\n",
      "avg / total       0.43      0.39      0.39      1170\n",
      "\n",
      "Test Vector Accuracy: 0.393162393162\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 2] seen 0 minibatches\n",
      "[epoch 2] seen 10 minibatches\n",
      "[epoch 2] seen 20 minibatches\n",
      "[epoch 2] seen 30 minibatches\n",
      "[epoch 2] Completed 37 minibatches in 0:21:16\n",
      "[epoch 2] Average cost: 0.706\n",
      "[epoch 2] Accuracy 0.824\n",
      "[epoch 2] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.77      0.84       630\n",
      "          1       0.57      0.85      0.68       631\n",
      "          2       0.86      0.78      0.82       629\n",
      "          3       0.89      0.87      0.88       619\n",
      "          4       0.93      0.83      0.88       620\n",
      "          5       0.97      0.84      0.90       614\n",
      "\n",
      "avg / total       0.85      0.82      0.83      3743\n",
      "\n",
      "Training Vector Accuracy:  0.823670852258\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 2] Validation Accuracy is 0.502\n",
      "[epoch 2] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.59      0.48      0.52       202\n",
      "          1       0.33      0.49      0.39       191\n",
      "          2       0.41      0.37      0.39       188\n",
      "          3       0.64      0.58      0.61       205\n",
      "          4       0.53      0.49      0.51       202\n",
      "          5       0.61      0.61      0.61       182\n",
      "\n",
      "avg / total       0.52      0.50      0.51      1170\n",
      "\n",
      "Test Vector Accuracy: 0.501709401709\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 3] seen 0 minibatches\n",
      "[epoch 3] seen 10 minibatches\n",
      "[epoch 3] seen 20 minibatches\n",
      "[epoch 3] seen 30 minibatches\n",
      "[epoch 3] Completed 37 minibatches in 0:21:12\n",
      "[epoch 3] Average cost: 0.498\n",
      "[epoch 3] Accuracy 0.868\n",
      "[epoch 3] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.82      0.87       630\n",
      "          1       0.84      0.80      0.82       631\n",
      "          2       0.67      0.94      0.78       629\n",
      "          3       0.93      0.92      0.93       619\n",
      "          4       0.96      0.86      0.90       620\n",
      "          5       0.98      0.87      0.92       614\n",
      "\n",
      "avg / total       0.89      0.87      0.87      3743\n",
      "\n",
      "Training Vector Accuracy:  0.867753139193\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 3] Validation Accuracy is 0.526\n",
      "[epoch 3] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.51      0.56       202\n",
      "          1       0.41      0.41      0.41       191\n",
      "          2       0.38      0.49      0.43       188\n",
      "          3       0.63      0.60      0.62       205\n",
      "          4       0.55      0.50      0.53       202\n",
      "          5       0.64      0.63      0.64       182\n",
      "\n",
      "avg / total       0.54      0.53      0.53      1170\n",
      "\n",
      "Test Vector Accuracy: 0.526495726496\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 4] seen 0 minibatches\n",
      "[epoch 4] seen 10 minibatches\n",
      "[epoch 4] seen 20 minibatches\n",
      "[epoch 4] seen 30 minibatches\n",
      "[epoch 4] Completed 37 minibatches in 0:21:18\n",
      "[epoch 4] Average cost: 0.404\n",
      "[epoch 4] Accuracy 0.888\n",
      "[epoch 4] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.86      0.90       630\n",
      "          1       0.68      0.89      0.77       631\n",
      "          2       0.89      0.88      0.88       629\n",
      "          3       0.95      0.94      0.94       619\n",
      "          4       0.97      0.87      0.92       620\n",
      "          5       0.99      0.89      0.94       614\n",
      "\n",
      "avg / total       0.90      0.89      0.89      3743\n",
      "\n",
      "Training Vector Accuracy:  0.887790542346\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 4] Validation Accuracy is 0.532\n",
      "[epoch 4] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.53      0.57       202\n",
      "          1       0.35      0.46      0.39       191\n",
      "          2       0.42      0.42      0.42       188\n",
      "          3       0.64      0.63      0.64       205\n",
      "          4       0.57      0.51      0.54       202\n",
      "          5       0.65      0.63      0.64       182\n",
      "\n",
      "avg / total       0.54      0.53      0.54      1170\n",
      "\n",
      "Test Vector Accuracy: 0.531623931624\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 5] seen 0 minibatches\n",
      "[epoch 5] seen 10 minibatches\n",
      "[epoch 5] seen 20 minibatches\n",
      "[epoch 5] seen 30 minibatches\n",
      "[epoch 5] Completed 37 minibatches in 0:21:14\n",
      "[epoch 5] Average cost: 0.352\n",
      "[epoch 5] Accuracy 0.898\n",
      "[epoch 5] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.88      0.91       630\n",
      "          1       0.70      0.91      0.79       631\n",
      "          2       0.90      0.89      0.90       629\n",
      "          3       0.96      0.94      0.95       619\n",
      "          4       0.97      0.87      0.92       620\n",
      "          5       0.99      0.90      0.94       614\n",
      "\n",
      "avg / total       0.91      0.90      0.90      3743\n",
      "\n",
      "Training Vector Accuracy:  0.897675661234\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 5] Validation Accuracy is 0.542\n",
      "[epoch 5] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.61      0.54      0.57       202\n",
      "          1       0.35      0.45      0.39       191\n",
      "          2       0.43      0.45      0.44       188\n",
      "          3       0.67      0.65      0.66       205\n",
      "          4       0.58      0.51      0.54       202\n",
      "          5       0.67      0.65      0.66       182\n",
      "\n",
      "avg / total       0.55      0.54      0.55      1170\n",
      "\n",
      "Test Vector Accuracy: 0.54188034188\n",
      "Model saved in file: /tmp/model\n"
     ]
    }
   ],
   "source": [
    "train_accur, test_accur, sess = run_epochs_cv(num_epoches = 5,\n",
    "                                              learning_rate = 0.5, \n",
    "                                              batch_size = 100,\n",
    "                                              min_rate = 0.1, \n",
    "                                              print_freq = 10, \n",
    "                                              seed = 42, \n",
    "                                              train_embedding = embedding_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2 (5 labeled + 2 unseen)\n",
    "\n",
    "### Remove 2 class and cross validate train data for hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_length: 500       \n",
      "num_classes: 7         \n",
      "vocabulary size: 20000     \n",
      "embedding size: 300       \n",
      "Train embedding?  False\n",
      "filter size: [3, 4, 5] \n",
      "number of filters: 150       \n",
      "L2 Regularization: 0.50      \n",
      "Keep drop out prob: 1.00      \n"
     ]
    }
   ],
   "source": [
    "doc_length = doc_length # s\n",
    "num_classes = num_class # M\n",
    "vocab_size = vocab.size\n",
    "embedding_size = 300 # d\n",
    "embedding_train = False # We use pretrained word2vec\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 150\n",
    "l2_reg_lambda = 0.5\n",
    "dropout_prob = 1.0\n",
    "display_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Training ------------\n",
      "[epoch 1] seen 0 minibatches\n",
      "[epoch 1] seen 10 minibatches\n",
      "[epoch 1] seen 20 minibatches\n",
      "[epoch 1] seen 30 minibatches\n",
      "[epoch 1] seen 40 minibatches\n",
      "[epoch 1] Completed 43 minibatches in 0:28:57\n",
      "[epoch 1] Average cost: 67.917\n",
      "[epoch 1] Accuracy 0.655\n",
      "[epoch 1] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.29      0.69      0.41       621\n",
      "          1       0.81      0.55      0.65       625\n",
      "          2       0.89      0.65      0.75       617\n",
      "          3       0.75      0.72      0.73       619\n",
      "          4       0.83      0.67      0.75       660\n",
      "          5       0.81      0.56      0.66       641\n",
      "          6       0.95      0.74      0.83       589\n",
      "\n",
      "avg / total       0.76      0.65      0.68      4372\n",
      "\n",
      "Training Vector Accuracy:  0.65462031107\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 1] Validation Accuracy is 0.400\n",
      "[epoch 1] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.22      0.55      0.32       200\n",
      "          1       0.37      0.25      0.30       218\n",
      "          2       0.59      0.44      0.51       199\n",
      "          3       0.60      0.53      0.56       200\n",
      "          4       0.45      0.37      0.41       174\n",
      "          5       0.26      0.16      0.20       198\n",
      "          6       0.64      0.52      0.58       178\n",
      "\n",
      "avg / total       0.45      0.40      0.41      1367\n",
      "\n",
      "Test Vector Accuracy: 0.400146305779\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 2] seen 0 minibatches\n",
      "[epoch 2] seen 10 minibatches\n",
      "[epoch 2] seen 20 minibatches\n",
      "[epoch 2] seen 30 minibatches\n",
      "[epoch 2] seen 40 minibatches\n",
      "[epoch 2] Completed 43 minibatches in 0:28:36\n",
      "[epoch 2] Average cost: 0.871\n",
      "[epoch 2] Accuracy 0.793\n",
      "[epoch 2] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.48      0.86      0.61       621\n",
      "          1       0.93      0.70      0.80       625\n",
      "          2       0.90      0.77      0.83       617\n",
      "          3       0.91      0.81      0.86       619\n",
      "          4       0.93      0.80      0.86       660\n",
      "          5       0.81      0.78      0.80       641\n",
      "          6       0.95      0.84      0.89       589\n",
      "\n",
      "avg / total       0.84      0.79      0.81      4372\n",
      "\n",
      "Training Vector Accuracy:  0.793458371455\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 2] Validation Accuracy is 0.470\n",
      "[epoch 2] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.29      0.58      0.39       200\n",
      "          1       0.45      0.31      0.37       218\n",
      "          2       0.69      0.47      0.56       199\n",
      "          3       0.65      0.56      0.60       200\n",
      "          4       0.49      0.46      0.47       174\n",
      "          5       0.34      0.32      0.33       198\n",
      "          6       0.67      0.61      0.64       178\n",
      "\n",
      "avg / total       0.51      0.47      0.48      1367\n",
      "\n",
      "Test Vector Accuracy: 0.469641550841\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 3] seen 0 minibatches\n",
      "[epoch 3] seen 10 minibatches\n",
      "[epoch 3] seen 20 minibatches\n",
      "[epoch 3] seen 30 minibatches\n",
      "[epoch 3] seen 40 minibatches\n",
      "[epoch 3] Completed 43 minibatches in 0:28:42\n",
      "[epoch 3] Average cost: 0.572\n",
      "[epoch 3] Accuracy 0.860\n",
      "[epoch 3] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.82      0.80       621\n",
      "          1       0.95      0.79      0.86       625\n",
      "          2       0.92      0.84      0.88       617\n",
      "          3       0.94      0.88      0.91       619\n",
      "          4       0.97      0.86      0.91       660\n",
      "          5       0.65      0.94      0.77       641\n",
      "          6       0.96      0.89      0.92       589\n",
      "\n",
      "avg / total       0.88      0.86      0.86      4372\n",
      "\n",
      "Training Vector Accuracy:  0.859789569991\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 3] Validation Accuracy is 0.476\n",
      "[epoch 3] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.47      0.43       200\n",
      "          1       0.45      0.33      0.38       218\n",
      "          2       0.60      0.49      0.54       199\n",
      "          3       0.65      0.55      0.59       200\n",
      "          4       0.49      0.48      0.48       174\n",
      "          5       0.28      0.44      0.34       198\n",
      "          6       0.69      0.61      0.64       178\n",
      "\n",
      "avg / total       0.50      0.48      0.48      1367\n",
      "\n",
      "Test Vector Accuracy: 0.4762253109\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 4] seen 0 minibatches\n",
      "[epoch 4] seen 10 minibatches\n",
      "[epoch 4] seen 20 minibatches\n",
      "[epoch 4] seen 30 minibatches\n",
      "[epoch 4] seen 40 minibatches\n",
      "[epoch 4] Completed 43 minibatches in 0:28:43\n",
      "[epoch 4] Average cost: 0.430\n",
      "[epoch 4] Accuracy 0.893\n",
      "[epoch 4] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.85      0.86       621\n",
      "          1       0.97      0.84      0.90       625\n",
      "          2       0.94      0.88      0.91       617\n",
      "          3       0.96      0.91      0.94       619\n",
      "          4       0.98      0.88      0.93       660\n",
      "          5       0.69      0.96      0.80       641\n",
      "          6       0.97      0.92      0.95       589\n",
      "\n",
      "avg / total       0.91      0.89      0.90      4372\n",
      "\n",
      "Training Vector Accuracy:  0.893412625801\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 4] Validation Accuracy is 0.489\n",
      "[epoch 4] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.48      0.47       200\n",
      "          1       0.46      0.35      0.40       218\n",
      "          2       0.60      0.50      0.55       199\n",
      "          3       0.64      0.55      0.59       200\n",
      "          4       0.48      0.47      0.47       174\n",
      "          5       0.29      0.48      0.36       198\n",
      "          6       0.69      0.63      0.66       178\n",
      "\n",
      "avg / total       0.51      0.49      0.50      1367\n",
      "\n",
      "Test Vector Accuracy: 0.489392831017\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 5] seen 0 minibatches\n",
      "[epoch 5] seen 10 minibatches\n",
      "[epoch 5] seen 20 minibatches\n",
      "[epoch 5] seen 30 minibatches\n",
      "[epoch 5] seen 40 minibatches\n",
      "[epoch 5] Completed 43 minibatches in 0:28:41\n",
      "[epoch 5] Average cost: 0.357\n",
      "[epoch 5] Accuracy 0.907\n",
      "[epoch 5] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.86      0.87       621\n",
      "          1       0.97      0.87      0.92       625\n",
      "          2       0.95      0.89      0.92       617\n",
      "          3       0.98      0.93      0.95       619\n",
      "          4       0.99      0.90      0.94       660\n",
      "          5       0.71      0.97      0.82       641\n",
      "          6       0.97      0.93      0.95       589\n",
      "\n",
      "avg / total       0.92      0.91      0.91      4372\n",
      "\n",
      "Training Vector Accuracy:  0.907136322049\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 5] Validation Accuracy is 0.495\n",
      "[epoch 5] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.47      0.46       200\n",
      "          1       0.48      0.38      0.42       218\n",
      "          2       0.60      0.51      0.55       199\n",
      "          3       0.63      0.56      0.59       200\n",
      "          4       0.48      0.47      0.47       174\n",
      "          5       0.30      0.46      0.36       198\n",
      "          6       0.70      0.65      0.67       178\n",
      "\n",
      "avg / total       0.52      0.50      0.50      1367\n",
      "\n",
      "Test Vector Accuracy: 0.49524506218\n",
      "Model saved in file: /tmp/model\n"
     ]
    }
   ],
   "source": [
    "train_accur, test_accur, sess = run_epochs_cv(num_epoches = 5,\n",
    "                                              learning_rate = 0.5, \n",
    "                                              batch_size = 100,\n",
    "                                              min_rate = 0.1, \n",
    "                                              print_freq = 10, \n",
    "                                              seed = 42, \n",
    "                                              train_embedding = embedding_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3 (5 labeled + 3 unseen)\n",
    "\n",
    "### Remove 2 class and cross validate train data for hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_length: 500       \n",
      "num_classes: 8         \n",
      "vocabulary size: 20000     \n",
      "embedding size: 300       \n",
      "Train embedding?  False\n",
      "filter size: [3, 4, 5] \n",
      "number of filters: 150       \n",
      "L2 Regularization: 0.50      \n",
      "Keep drop out prob: 1.00      \n"
     ]
    }
   ],
   "source": [
    "doc_length = doc_length # s\n",
    "num_classes = num_class # M\n",
    "vocab_size = vocab.size\n",
    "embedding_size = 300 # d\n",
    "embedding_train = False # We use pretrained word2vec\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 150\n",
    "l2_reg_lambda = 0.5\n",
    "dropout_prob = 1.0\n",
    "display_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------- Training ------------\n",
      "[epoch 1] seen 0 minibatches\n",
      "[epoch 1] seen 10 minibatches\n",
      "[epoch 1] seen 20 minibatches\n",
      "[epoch 1] seen 30 minibatches\n",
      "[epoch 1] seen 40 minibatches\n",
      "[epoch 1] Completed 49 minibatches in 0:36:50\n",
      "[epoch 1] Average cost: 62.546\n",
      "[epoch 1] Accuracy 0.643\n",
      "[epoch 1] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.55      0.63       643\n",
      "          1       0.28      0.71      0.40       624\n",
      "          2       0.87      0.54      0.67       606\n",
      "          3       0.83      0.79      0.81       658\n",
      "          4       0.73      0.67      0.70       639\n",
      "          5       0.82      0.55      0.66       615\n",
      "          6       0.81      0.65      0.72       594\n",
      "          7       0.94      0.66      0.78       576\n",
      "\n",
      "avg / total       0.75      0.64      0.67      4955\n",
      "\n",
      "Training Vector Accuracy:  0.642583249243\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 1] Validation Accuracy is 0.356\n",
      "[epoch 1] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.34      0.20      0.25       194\n",
      "          1       0.21      0.51      0.30       206\n",
      "          2       0.52      0.27      0.36       199\n",
      "          3       0.52      0.58      0.55       187\n",
      "          4       0.29      0.38      0.33       192\n",
      "          5       0.34      0.21      0.26       200\n",
      "          6       0.41      0.36      0.39       160\n",
      "          7       0.61      0.34      0.43       211\n",
      "\n",
      "avg / total       0.41      0.36      0.36      1549\n",
      "\n",
      "Test Vector Accuracy: 0.356358941252\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 2] seen 0 minibatches\n",
      "[epoch 2] seen 10 minibatches\n",
      "[epoch 2] seen 20 minibatches\n",
      "[epoch 2] seen 30 minibatches\n",
      "[epoch 2] seen 40 minibatches\n",
      "[epoch 2] Completed 49 minibatches in 0:36:52\n",
      "[epoch 2] Average cost: 0.882\n",
      "[epoch 2] Accuracy 0.812\n",
      "[epoch 2] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.81      0.80       643\n",
      "          1       0.77      0.72      0.74       624\n",
      "          2       0.63      0.85      0.73       606\n",
      "          3       0.94      0.88      0.91       658\n",
      "          4       0.93      0.79      0.85       639\n",
      "          5       0.73      0.81      0.77       615\n",
      "          6       0.90      0.82      0.86       594\n",
      "          7       0.91      0.82      0.86       576\n",
      "\n",
      "avg / total       0.83      0.81      0.82      4955\n",
      "\n",
      "Training Vector Accuracy:  0.812310797175\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 2] Validation Accuracy is 0.436\n",
      "[epoch 2] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.42      0.41       194\n",
      "          1       0.30      0.31      0.31       206\n",
      "          2       0.37      0.41      0.39       199\n",
      "          3       0.61      0.67      0.64       187\n",
      "          4       0.43      0.46      0.44       192\n",
      "          5       0.36      0.37      0.36       200\n",
      "          6       0.46      0.42      0.44       160\n",
      "          7       0.60      0.44      0.51       211\n",
      "\n",
      "avg / total       0.44      0.44      0.44      1549\n",
      "\n",
      "Test Vector Accuracy: 0.435765009684\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 3] seen 0 minibatches\n",
      "[epoch 3] seen 10 minibatches\n",
      "[epoch 3] seen 20 minibatches\n",
      "[epoch 3] seen 30 minibatches\n",
      "[epoch 3] seen 40 minibatches\n",
      "[epoch 3] Completed 49 minibatches in 0:36:57\n",
      "[epoch 3] Average cost: 0.561\n",
      "[epoch 3] Accuracy 0.879\n",
      "[epoch 3] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.87      0.87       643\n",
      "          1       0.90      0.79      0.84       624\n",
      "          2       0.69      0.94      0.79       606\n",
      "          3       0.97      0.91      0.94       658\n",
      "          4       0.96      0.86      0.91       639\n",
      "          5       0.82      0.90      0.86       615\n",
      "          6       0.95      0.89      0.92       594\n",
      "          7       0.95      0.88      0.91       576\n",
      "\n",
      "avg / total       0.89      0.88      0.88      4955\n",
      "\n",
      "Training Vector Accuracy:  0.879112008073\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 3] Validation Accuracy is 0.460\n",
      "[epoch 3] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.47      0.47      0.47       194\n",
      "          1       0.34      0.33      0.33       206\n",
      "          2       0.40      0.45      0.42       199\n",
      "          3       0.63      0.65      0.64       187\n",
      "          4       0.44      0.50      0.47       192\n",
      "          5       0.37      0.39      0.38       200\n",
      "          6       0.46      0.42      0.44       160\n",
      "          7       0.59      0.48      0.53       211\n",
      "\n",
      "avg / total       0.46      0.46      0.46      1549\n",
      "\n",
      "Test Vector Accuracy: 0.459651387992\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 4] seen 0 minibatches\n",
      "[epoch 4] seen 10 minibatches\n",
      "[epoch 4] seen 20 minibatches\n",
      "[epoch 4] seen 30 minibatches\n",
      "[epoch 4] seen 40 minibatches\n",
      "[epoch 4] Completed 49 minibatches in 0:36:49\n",
      "[epoch 4] Average cost: 0.418\n",
      "[epoch 4] Accuracy 0.907\n",
      "[epoch 4] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.91      0.90      0.90       643\n",
      "          1       0.94      0.83      0.88       624\n",
      "          2       0.71      0.97      0.82       606\n",
      "          3       0.99      0.92      0.95       658\n",
      "          4       0.98      0.90      0.93       639\n",
      "          5       0.89      0.93      0.91       615\n",
      "          6       0.96      0.91      0.94       594\n",
      "          7       0.96      0.90      0.93       576\n",
      "\n",
      "avg / total       0.92      0.91      0.91      4955\n",
      "\n",
      "Training Vector Accuracy:  0.906559031282\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 4] Validation Accuracy is 0.481\n",
      "[epoch 4] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.47      0.49       194\n",
      "          1       0.39      0.37      0.38       206\n",
      "          2       0.43      0.47      0.45       199\n",
      "          3       0.64      0.67      0.66       187\n",
      "          4       0.46      0.52      0.48       192\n",
      "          5       0.39      0.41      0.40       200\n",
      "          6       0.46      0.44      0.45       160\n",
      "          7       0.60      0.49      0.54       211\n",
      "\n",
      "avg / total       0.48      0.48      0.48      1549\n",
      "\n",
      "Test Vector Accuracy: 0.480955455132\n",
      "\n",
      "----------- Training ------------\n",
      "[epoch 5] seen 0 minibatches\n",
      "[epoch 5] seen 10 minibatches\n",
      "[epoch 5] seen 20 minibatches\n",
      "[epoch 5] seen 30 minibatches\n",
      "[epoch 5] seen 40 minibatches\n",
      "[epoch 5] Completed 49 minibatches in 0:36:54\n",
      "[epoch 5] Average cost: 0.347\n",
      "[epoch 5] Accuracy 0.920\n",
      "[epoch 5] Training Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.91      0.92       643\n",
      "          1       0.96      0.86      0.90       624\n",
      "          2       0.72      0.98      0.83       606\n",
      "          3       0.98      0.93      0.96       658\n",
      "          4       0.98      0.91      0.94       639\n",
      "          5       0.91      0.94      0.92       615\n",
      "          6       0.98      0.92      0.95       594\n",
      "          7       0.97      0.92      0.95       576\n",
      "\n",
      "avg / total       0.93      0.92      0.92      4955\n",
      "\n",
      "Training Vector Accuracy:  0.920484359233\n",
      "\n",
      "---------- Validation ----------\n",
      "[epoch 5] Validation Accuracy is 0.485\n",
      "[epoch 5] Validation Classificaiton Report\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.51      0.47      0.49       194\n",
      "          1       0.39      0.37      0.38       206\n",
      "          2       0.43      0.48      0.45       199\n",
      "          3       0.65      0.67      0.66       187\n",
      "          4       0.46      0.51      0.48       192\n",
      "          5       0.41      0.42      0.41       200\n",
      "          6       0.46      0.46      0.46       160\n",
      "          7       0.60      0.51      0.55       211\n",
      "\n",
      "avg / total       0.49      0.49      0.49      1549\n",
      "\n",
      "Test Vector Accuracy: 0.485474499677\n",
      "Model saved in file: /tmp/model\n"
     ]
    }
   ],
   "source": [
    "train_accur, test_accur, sess = run_epochs_cv(num_epoches = 5,\n",
    "                                              learning_rate = 0.5, \n",
    "                                              batch_size = 100,\n",
    "                                              min_rate = 0.1, \n",
    "                                              print_freq = 10, \n",
    "                                              seed = 42, \n",
    "                                              train_embedding = embedding_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
