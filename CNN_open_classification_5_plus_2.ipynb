{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Classification with CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'shared_lib.utils' from 'shared_lib/utils.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, sys, re, json, time, shutil\n",
    "import itertools, collections\n",
    "from IPython.display import display, HTML\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.1\"))\n",
    "\n",
    "# utils.pretty_print_matrix uses Pandas. Configure float format here.\n",
    "import pandas as pd\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries\n",
    "from shared_lib import utils, vocabulary, tf_embed_viz\n",
    "\n",
    "import copy\n",
    "\n",
    "# Import model\n",
    "#import cnnlm\n",
    "reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load SKlearn libraries\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'Data_Set/'\n",
    "PROJECT_PATH = os.getcwd()\n",
    "PROJECT_DATA = os.path.join(PROJECT_PATH, DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class: 0\t = alt.atheism\n",
      "class: 1\t = comp.graphics\n",
      "class: 2\t = comp.os.ms-windows.misc\n",
      "class: 3\t = comp.sys.ibm.pc.hardware\n",
      "class: 4\t = comp.sys.mac.hardware\n",
      "class: 5\t = comp.windows.x\n",
      "class: 6\t = misc.forsale\n",
      "class: 7\t = rec.autos\n",
      "class: 8\t = rec.motorcycles\n",
      "class: 9\t = rec.sport.baseball\n",
      "class: 10\t = rec.sport.hockey\n",
      "class: 11\t = sci.crypt\n",
      "class: 12\t = sci.electronics\n",
      "class: 13\t = sci.med\n",
      "class: 14\t = sci.space\n",
      "class: 15\t = soc.religion.christian\n",
      "class: 16\t = talk.politics.guns\n",
      "class: 17\t = talk.politics.mideast\n",
      "class: 18\t = talk.politics.misc\n",
      "class: 19\t = talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# Get newsgroup data\n",
    "newsgroup_data_all = fetch_20newsgroups(subset = 'all', remove=('headers', 'footers', 'quotes'))\n",
    "all_data, all_labels = newsgroup_data_all.data, newsgroup_data_all.target\n",
    "\n",
    "# List of all the class labels\n",
    "label_list = list(newsgroup_data_all.target_names)\n",
    "\n",
    "# Print the class labels\n",
    "i = 0\n",
    "for label in label_list:\n",
    "    print \"class: %i\\t = %s\" %(i, label)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newsgroup_all.txt\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "f = open('./Data_Set/newsgroup_prep/newsgroup_all.txt', 'w') \n",
    "for doc in all_data:\n",
    "    # Clean up str\n",
    "    doc = utils.clean_str((doc).encode('utf-8'))\n",
    "    # remove stop words and do stemming optionaly\n",
    "    doc = utils.preprocess_stop_stem(doc, stop=True, sent=True, stem=False)\n",
    "    f.write(\"%s\\n\" %(doc))\n",
    "f.close()\n",
    "\n",
    "# RegEx or list of file names\n",
    "data_20newsgroup = os.path.join(PROJECT_DATA, 'newsgroup_prep/')\n",
    "\n",
    "corpus = PlaintextCorpusReader(data_20newsgroup, 'newsgroup_all.txt')\n",
    "\n",
    "for infile in sorted(corpus.fileids()):\n",
    "    print infile # The fileids of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: 20000 words\n"
     ]
    }
   ],
   "source": [
    "V = 20000\n",
    "vocab = vocabulary.Vocabulary((utils.canonicalize_word(w) \n",
    "                               for w in utils.flatten(corpus.sents())),\n",
    "                               size = V)\n",
    "print \"Vocabulary: %d words\" % vocab.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly Select Classes:  ['comp.windows.x', 'talk.politics.mideast', 'comp.sys.ibm.pc.hardware', 'sci.crypt', 'comp.os.ms-windows.misc', 'comp.sys.mac.hardware', 'sci.electronics']\n",
      "[0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Select training and test data based on the number of classes\n",
    "# Including randomization option\n",
    "import random\n",
    "from random import randint\n",
    "random.seed(8)\n",
    "\n",
    "num_class = 7\n",
    "randomize = True\n",
    "\n",
    "if randomize == True:\n",
    "    label_idxs = []\n",
    "    label_idxs = random.sample(range(1, 19), num_class)\n",
    "else:\n",
    "    label_idxs = range(num_class)\n",
    "\n",
    "select_classes = [label_list[i] for i in label_idxs]\n",
    "print \"Randomly Select Classes: \", select_classes\n",
    "\n",
    "newsgroups_all = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'),\n",
    "                                    categories=select_classes)\n",
    "\n",
    "all_data, all_labels = newsgroups_all.data, newsgroups_all.target\n",
    "print np.unique(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6833 docs (2.08186e+07 tokens)\n",
      "Training set: 5466 docs (16656722 tokens)\n",
      "Test set: 1367 docs (4161876 tokens)\n"
     ]
    }
   ],
   "source": [
    "doc_length = 500\n",
    "\n",
    "# Preprocess data\n",
    "# Cleaning special characters\n",
    "# Cut or pad based on document length\n",
    "all_docs = utils.preprocess_doc(all_data, length = doc_length)\n",
    "\n",
    "# Split total data set to training and test set\n",
    "train_docs, train_labels, test_docs, test_labels = utils.get_train_test_docs(all_docs, \n",
    "                                                                             all_labels, \n",
    "                                                                             split = 0.8, \n",
    "                                                                             shuffle = True)\n",
    "orig_test_labels = copy.copy(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Training Docs shape: (5466, 500) should equal to (batch_size, doc_length)\n",
      "Input Training labels shape: (5466, 7) should equal to (batch_size, num_class)\n",
      "Input Testing Docs shape: (1367, 500) should equal to (batch_size, doc_length)\n",
      "Input Testing labels shape: (1367, 7) should equal to (batch_size, num_class)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize documents and conver to ID\n",
    "# We tokenize each docs in the dataset and convert to vocab ID\n",
    "# matrix of batch_size x doc_length\n",
    "train_docs_ids = utils.docs_to_ids(train_docs, vocab)\n",
    "test_docs_ids = utils.docs_to_ids(test_docs, vocab)\n",
    "\n",
    "# Convert label to one-hot-code\n",
    "train_labels_oh = np.eye(num_class)[train_labels]\n",
    "test_labels_oh = np.eye(num_class)[test_labels]\n",
    "\n",
    "\n",
    "print \"Input Training Docs shape:\", train_docs_ids.shape, \"should equal to (batch_size, doc_length)\"\n",
    "print \"Input Training labels shape:\", train_labels_oh.shape, \"should equal to (batch_size, num_class)\"\n",
    "print \"Input Testing Docs shape:\", test_docs_ids.shape, \"should equal to (batch_size, doc_length)\"\n",
    "print \"Input Testing labels shape:\", test_labels_oh.shape, \"should equal to (batch_size, num_class)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Pretrained Google Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_google_bin(fname, vocab):\n",
    "    \"\"\"\n",
    "    Loads 300x1 word vecs from Google (Mikolov) word2vec\n",
    "    \"\"\"\n",
    "    word_vecs = {}\n",
    "    with open(fname, \"rb\") as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, layer1_size = map(int, header.split())\n",
    "        print \"Google Word2vec Vocabulary Size:\", vocab_size\n",
    "        print \"Vector size:\", layer1_size\n",
    "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
    "        print \"Binary Length of word vector:\", binary_len\n",
    "        for line in xrange(vocab_size):\n",
    "            word = []\n",
    "            while True: # Read 1 char a time\n",
    "                ch = f.read(1) \n",
    "                if ch == ' ': # If it is a space, a word is read, we join then to read its vector\n",
    "                    word = ''.join(word)\n",
    "                    break\n",
    "                if ch != '\\n': # If it is not \\n, grouping character\n",
    "                    word.append(ch) \n",
    "            if word in vocab.wordset: # If a word in the 20 newsgroup vocab, get its vector\n",
    "                word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    f.close()\n",
    "    return word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Word2vec Vocabulary Size: 3000000\n",
      "Vector size: 300\n",
      "Binary Length of word vector: 1200\n"
     ]
    }
   ],
   "source": [
    "google_word2vec = load_google_bin('./google_word2vec/GoogleNews-vectors-negative300.bin', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of vocabulary in 20newsgroup: 20000\n",
      "Total matched vocabulary from google word2vec: 16555\n",
      "--- Print a sample of google_word2vec vocabulary ---\n",
      "Word: raining \t\t Vector: [ 0.02331543  0.05004883 -0.00059891] ...\n",
      "Word: writings \t\t Vector: [ 0.18945312  0.2109375   0.20507812] ...\n",
      "Word: divinely \t\t Vector: [-0.02783203 -0.40820312 -0.01037598] ...\n",
      "Word: foul \t\t Vector: [ 0.18847656 -0.28710938  0.33007812] ...\n",
      "Word: four \t\t Vector: [ 0.0859375  -0.07275391  0.01672363] ...\n",
      "Word: gag \t\t Vector: [ 0.14648438 -0.08203125 -0.00897217] ...\n",
      "Word: prefix \t\t Vector: [ 0.34570312  0.1640625   0.11425781] ...\n",
      "Word: woods \t\t Vector: [ 0.11328125 -0.01165771 -0.20800781] ...\n",
      "Word: verses \t\t Vector: [ 0.28710938  0.15820312  0.23828125] ...\n",
      "Word: hanging \t\t Vector: [ 0.08984375  0.13769531 -0.14941406] ...\n",
      "Word: woody \t\t Vector: [ 0.08251953  0.44140625  0.07421875] ...\n"
     ]
    }
   ],
   "source": [
    "print \"Total Number of vocabulary in 20newsgroup:\", vocab.size\n",
    "print \"Total matched vocabulary from google word2vec:\", len(google_word2vec.keys())\n",
    "print \"--- Print a sample of google_word2vec vocabulary ---\"\n",
    "i = 0\n",
    "for k, v in google_word2vec.iteritems():\n",
    "    if i <= 10:\n",
    "        print \"Word: %s \\t\\t Vector: %s ...\" %(k, v[:3])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_unknown_words(google_word2vec, vocab, k=300):\n",
    "    for word in vocab.wordset:\n",
    "        if word not in google_word2vec:\n",
    "            google_word2vec[word] = np.random.uniform(-0.25,0.25,k)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of vocabulary in 20newsgroup: 20000\n",
      "Total matched vocabulary from google word2vec: 20000\n",
      "Pre-trained word2vec size (20000, 300)\n"
     ]
    }
   ],
   "source": [
    "add_unknown_words(google_word2vec, vocab, k=300)\n",
    "print \"Total Number of vocabulary in 20newsgroup:\", vocab.size\n",
    "print \"Total matched vocabulary from google word2vec:\", len(google_word2vec.keys())\n",
    "pt_word2vec = np.array(google_word2vec.values())\n",
    "print \"Pre-trained word2vec size\", pt_word2vec.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_length = 500 # s\n",
    "num_classes = num_class # M\n",
    "vocab_size = 20000\n",
    "embedding_size = 300 # d\n",
    "embedding_train = False # We use pretrained word2vec\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 150\n",
    "l2_reg_lambda = 0.5\n",
    "dropout_prob = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for input, output and dropout\n",
    "# x_: Document, Size: (batch, document_length) word in indice\n",
    "# y_: Classes, Size: (batch, num_of_classes)\n",
    "# dropout_keep_prob: Dropout regularization parameter\n",
    "x_ = tf.placeholder(tf.int32, [None, doc_length], name=\"x\")\n",
    "y_ = tf.placeholder(tf.float32, [None, num_classes], name=\"y\")\n",
    "dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "# Keeping track of l2 regularization loss (optional)\n",
    "l2_loss = tf.constant(0.0)\n",
    "\n",
    "# Embedding layer (Train embedding layer)\n",
    "# Need different implementation if use google pretrained word2vec\n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "    # The vocab to vector table for lookup (to be trained or pre-trained)\n",
    "    if embedding_train:\n",
    "        C_ = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0), name=\"C\")\n",
    "    else:\n",
    "        C_ = tf.placeholder(tf.float32, [vocab_size, embedding_size], name=\"C\")\n",
    "\n",
    "    # Embedding output needs to be in size: (batch, doc_length, embedding_size, 1)\n",
    "    # Lookup gives (batch, doc_length, embedding_size)\n",
    "    # Therefore, we need to expand the dimension to 4D to work with conv2d\n",
    "    embedded_out = tf.expand_dims(tf.nn.embedding_lookup(C_, x_), -1)\n",
    "\n",
    "# Create a convolution + maxpool layer for each filter size\n",
    "pooled_outputs = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "\n",
    "        # Convolution Layer\n",
    "        # input shape: (batch, height(doc length, width(embedding size), channels(1) )\n",
    "        # filter shape: (filter_height, filter width(same as embedding size), in_channel, out_channels)\n",
    "        # in_channel = 1 for our data\n",
    "        # out_channel = num_filters\n",
    "        filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "\n",
    "        # To experiment with normal distribution\n",
    "        W_ = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "        b_ = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "\n",
    "        # \"VALID\" padding means no padding at edge\n",
    "        # Return shape (batch, height(doc length, width(embedding size), 1)\n",
    "        conv_ = tf.nn.conv2d(embedded_out, W_, strides=[1, 1, 1, 1], padding=\"VALID\", name=\"conv\")\n",
    "\n",
    "        # Apply nonlinearity using Relu (train fasster than tanh)\n",
    "        # Return shape (batch, height(doc length, 1, 1)\n",
    "        h_ = tf.nn.relu(tf.nn.bias_add(conv_, b_), name=\"relu\")\n",
    "\n",
    "        # Maxpooling over the outputs\n",
    "        # ksize is window for pooling, we took 1 value for width direction\n",
    "        # For height, apply to each convolution steps to stripe the whole input matrix.\n",
    "        # Return shape (1, doc_length-filter_size+1, 1, 1)\n",
    "        pooled = tf.nn.max_pool(h_, \n",
    "                                ksize=[1, doc_length - filter_size + 1, 1, 1],\n",
    "                                strides=[1, 1, 1, 1], \n",
    "                                padding='VALID', \n",
    "                                name=\"pool\")\n",
    "        pooled_outputs.append(pooled)\n",
    "\n",
    "# Combine all the pooled features\n",
    "# find the total number of filters = num_of_filters * num_of_region\n",
    "# If we use [2, 3, 4] and 2 filter per region, we have 3 * 2 = 6 filters\n",
    "num_filters_total = num_filters * len(filter_sizes)\n",
    "\n",
    "# combine pooling output to feature vectors\n",
    "# h_pool_flat in shape of (batch_size, ? , num_filters_total)\n",
    "h_pool = tf.concat(pooled_outputs, 3)\n",
    "h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n",
    "\n",
    "# Add dropout\n",
    "with tf.name_scope(\"dropout\"):\n",
    "    h_drop = tf.nn.dropout(h_pool_flat, dropout_keep_prob)\n",
    "\n",
    "# Output Layer: Softmax\n",
    "# Final (unnormalized) scores and predictions\n",
    "# Do we need to normalize?\n",
    "with tf.name_scope(\"Output_layer\"):\n",
    "    Z_ = tf.Variable(tf.random_uniform([num_filters_total, num_classes], -1.0, 1.0), name = \"Z\")\n",
    "    b_output_ = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b_output\")\n",
    "    logits_ = tf.add(tf.matmul(h_drop, Z_), b_output_, name=\"logits\")\n",
    "\n",
    "    # L2 loss\n",
    "    l2_loss += tf.nn.l2_loss(Z_)\n",
    "    l2_loss += tf.nn.l2_loss(b_)\n",
    "\n",
    "    #scores = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\n",
    "    predictions_ = tf.argmax(logits_, 1, name=\"predictions\")\n",
    "\n",
    "# Calculate mean cross-entropy loss\n",
    "with tf.name_scope(\"cost_function\"):\n",
    "    per_example_losses_ = tf.nn.softmax_cross_entropy_with_logits(logits=logits_, \n",
    "                                                                 labels=y_,\n",
    "                                                                 name=\"per_example_loss\")\n",
    "    loss_ = tf.reduce_mean(per_example_losses_) + l2_reg_lambda * l2_loss\n",
    "\n",
    "# Accuracy\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_predictions_ = tf.equal(predictions_, tf.argmax(y_, 1))\n",
    "    accuracy_ = tf.reduce_mean(tf.cast(correct_predictions_, \"float\"), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"Training\"):\n",
    "    alpha_ = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    optimizer_ = tf.train.AdagradOptimizer(alpha_)\n",
    "    #optimizer_ = tf.train.AdamOptimizer(alpha_)\n",
    "    train_step_ = optimizer_.minimize(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model (5 labeled + 2 unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/model\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"./CNN_5p2_model/model\")\n",
    "    print(\"Model restored.\")\n",
    "    feed_dict_train = {x_:train_docs_ids,\n",
    "                   y_:train_labels_oh,\n",
    "                   C_:pt_word2vec,\n",
    "                   dropout_keep_prob:dropout_prob}\n",
    "    train_vectors = sess.run([h_drop], feed_dict=feed_dict_train)[0]\n",
    "    feed_dict_test = {x_:test_docs_ids,\n",
    "                   y_:test_labels_oh,\n",
    "                   C_:pt_word2vec,\n",
    "                   dropout_keep_prob:dropout_prob}\n",
    "    test_vectors = sess.run([h_drop], feed_dict=feed_dict_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_vector shape:  (5466, 450)\n",
      "test_vector shape:  (1367, 450)\n",
      "train_label shape:  (5466,)\n",
      "test_label shape:  (1367,)\n"
     ]
    }
   ],
   "source": [
    "print \"train_vector shape: \", train_vectors.shape\n",
    "print \"test_vector shape: \", test_vectors.shape\n",
    "print \"train_label shape: \", train_labels.shape\n",
    "print \"test_label shape: \", test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-vs-Rest Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5466, 450)\n",
      "(3113, 450)\n",
      "(3113,)\n",
      "(1094, 450)\n",
      "(1094,)\n",
      "(780, 450)\n",
      "(780,)\n",
      "(1367, 450)\n",
      "(1367,)\n",
      "[1 2 4 5 6]\n",
      "[0 1 2 3 4 5 6]\n",
      "[1 2 4 5 6]\n",
      "[0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "train_valid_cut = int(len(train_vectors)*0.8)\n",
    "valid_final_vectors=train_vectors[train_valid_cut:]\n",
    "valid_final_labels = train_labels[train_valid_cut:]\n",
    "\n",
    "train_new_vectors = train_vectors[:train_valid_cut]\n",
    "train_new_labels = train_labels[:train_valid_cut]\n",
    "\n",
    "missing_class = np.array([0, 3])\n",
    "missing_class_idx = np.where(np.isin(train_new_labels, missing_class))[0]\n",
    "train_final_vectors = [train_new_vectors[i] for i in range(len(train_new_vectors)) if i not in missing_class_idx]\n",
    "train_final_labels = [train_new_labels[i] for i in range(len(train_new_labels)) if i not in missing_class_idx]\n",
    "\n",
    "val_missing_class_idx = np.where(np.isin(valid_final_labels, missing_class))[0]\n",
    "valid_calib_vectors = [valid_final_vectors[i] for i in range(len(valid_final_vectors)) if i not in val_missing_class_idx]\n",
    "valid_calib_labels = [valid_final_labels[i] for i in range(len(valid_final_labels)) if i not in val_missing_class_idx]\n",
    "\n",
    "\n",
    "\n",
    "print np.array(train_vectors).shape\n",
    "print np.array(train_final_vectors).shape\n",
    "print np.array(train_final_labels).shape\n",
    "\n",
    "print np.array(valid_final_vectors).shape\n",
    "print np.array(valid_final_labels).shape\n",
    "\n",
    "print np.array(valid_calib_vectors).shape\n",
    "print np.array(valid_calib_labels).shape\n",
    "\n",
    "\n",
    "print np.array(test_vectors).shape\n",
    "print np.array(test_labels).shape\n",
    "print np.unique(train_final_labels)\n",
    "print np.unique(valid_final_labels)\n",
    "print np.unique(valid_calib_labels)\n",
    "print np.unique(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1-vs-Rest SVM ---\n",
      "Fitting 3 folds for each of 36 candidates, totalling 108 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 108 out of 108 | elapsed: 10.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.538408152371\n",
      "{'estimator__kernel': 'linear', 'estimator__C': 0.5, 'estimator__degree': 1}\n"
     ]
    }
   ],
   "source": [
    "# Method SVM 1-vs-Rest\n",
    "clf_svc = OneVsRestClassifier(SVC(probability=True))\n",
    "clf_svc.fit(train_final_vectors, train_final_labels)\n",
    "\n",
    "parameters_SVC = {\n",
    "    \"estimator__C\": [0.5, 1, 2],\n",
    "    \"estimator__kernel\": [\"poly\",\"rbf\", \"sigmoid\", \"linear\"],\n",
    "    \"estimator__degree\":[1, 2, 3],\n",
    "}\n",
    "\n",
    "print \"--- 1-vs-Rest SVM ---\"\n",
    "mod_svc = GridSearchCV(estimator=clf_svc, param_grid=parameters_SVC, scoring='f1_macro', verbose=True)\n",
    "mod_svc.fit(valid_final_vectors, valid_final_labels)\n",
    "print mod_svc.best_score_ \n",
    "print mod_svc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 4 5 6]\n",
      "[0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print np.unique(train_final_labels)\n",
    "print np.unique(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda2/lib/python2.7/site-packages/sklearn/calibration.py:435: RuntimeWarning: overflow encountered in exp\n",
      "  E = np.exp(AB[0] * F + AB[1])\n",
      "/home/nlp/anaconda2/lib/python2.7/site-packages/sklearn/calibration.py:445: RuntimeWarning: overflow encountered in exp\n",
      "  E = np.exp(AB[0] * F + AB[1])\n",
      "/home/nlp/anaconda2/lib/python2.7/site-packages/sklearn/calibration.py:447: RuntimeWarning: invalid value encountered in multiply\n",
      "  TEP_minus_T1P = P * (T * E - T1)\n"
     ]
    }
   ],
   "source": [
    "clf_svc = OneVsRestClassifier(SVC(probability=True, \n",
    "                              kernel=mod_svc.best_params_['estimator__kernel'], \n",
    "                              C=mod_svc.best_params_['estimator__C'], \n",
    "                              degree=mod_svc.best_params_['estimator__degree']))\n",
    "\n",
    "clf_svc.fit(train_final_vectors, train_final_labels)\n",
    "\n",
    "## Calibrate prob\n",
    "sig_clf_svc = CalibratedClassifierCV(clf_svc, method=\"sigmoid\", cv=\"prefit\")\n",
    "sig_clf_svc.fit(valid_final_vectors, valid_final_labels)\n",
    "\n",
    "sig_clf_svc_probs = sig_clf_svc.predict_proba(test_vectors)\n",
    "\n",
    "\n",
    "# Manual Calibration\n",
    "clf_svc_probs = clf_svc.predict_proba(test_vectors)\n",
    "\n",
    "class_max_prob_lists = np.array([max(clf_svc_probs[:,val]) for val in range(len(clf_svc_probs[0]))])\n",
    "class_min_prob_lists = np.array([min(clf_svc_probs[:,val]) for val in range(len(clf_svc_probs[0]))])\n",
    "delta = class_max_prob_lists - class_min_prob_lists\n",
    "\n",
    "\n",
    "# Normalized for test vector\n",
    "scaled_calib_svc_probs = np.divide(clf_svc_probs,delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1089efe490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.amax(scaled_calib_svc_probs, axis = 1)\n",
    "plt.figure(figsize = (12, 8))\n",
    "n, bins, patches = plt.hist(x, 50)\n",
    "plt.title(\"max prob dist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 4 5 6]\n",
      "[1 2 4 5 6]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       200\n",
      "          1       0.34      0.48      0.40       218\n",
      "          2       0.42      0.56      0.48       199\n",
      "          3       0.00      0.00      0.00       200\n",
      "          4       0.35      0.41      0.38       174\n",
      "          5       0.25      0.46      0.33       198\n",
      "          6       0.50      0.62      0.55       178\n",
      "\n",
      "avg / total       0.26      0.36      0.30      1367\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00       200\n",
      "          1       0.32      0.47      0.38       218\n",
      "          2       0.43      0.53      0.47       199\n",
      "          3       0.00      0.00      0.00       200\n",
      "          4       0.28      0.50      0.36       174\n",
      "          5       0.29      0.40      0.34       198\n",
      "          6       0.50      0.60      0.54       178\n",
      "\n",
      "avg / total       0.26      0.35      0.29      1367\n",
      "\n",
      "0.357717629846\n",
      "0.351865398683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nlp/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "preds1 = clf_svc.predict(test_vectors)\n",
    "preds2 = sig_clf_svc.predict(test_vectors)\n",
    "print np.unique(preds1)\n",
    "print np.unique(preds2)\n",
    "print classification_report(test_labels,preds1)\n",
    "print classification_report(test_labels,preds2)\n",
    "print accuracy_score(test_labels,preds1)\n",
    "print accuracy_score(test_labels,preds2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To find the new class, we select a Percentile for Probability Threshold for Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_prob_percentile_85 = np.array([np.percentile(scaled_calib_svc_probs[:,val], 85.0) \n",
    "                                     for val in range(len(scaled_calib_svc_probs[0]))])\n",
    "\n",
    "test_class_preds = np.greater_equal(scaled_calib_svc_probs,class_prob_percentile_85).astype(int)\n",
    "\n",
    "# Predict the test label based on percental\n",
    "# If a data is below 80% for prob of all class, it belongs to open class\n",
    "valid_class_probs = np.multiply(scaled_calib_svc_probs, test_class_preds)\n",
    "valid_class = np.greater_equal(np.ceil(valid_class_probs),1).astype(int)\n",
    "predicted_multinomial = np.multiply(valid_class, np.unique(train_final_labels))\n",
    "predicted_test_class = np.max(predicted_multinomial,axis=1)\n",
    "\n",
    "# The true unseen class index\n",
    "unseen_class_indices = np.where(np.isin(test_labels, missing_class))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted test classes [0 1 2 4 5 6]\n",
      "train labels [1 2 4 5 6]\n",
      "Full list of original test labels [0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print \"predicted test classes\", np.unique(predicted_test_class)\n",
    "print \"train labels\", np.unique(train_final_labels)\n",
    "print \"Full list of original test labels\", np.unique(orig_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "orig_valid_labels = copy.copy(valid_final_labels)\n",
    "\n",
    "missing_class_idx_test = np.where(np.isin(test_labels, missing_class))[0]\n",
    "missing_class_idx_val = np.where(np.isin(valid_final_labels, missing_class))[0]\n",
    "\n",
    "#print missing_class_idx_test\n",
    "#orig_test_labels = copy.copy(test_labels)\n",
    "for i in range(len(test_labels)): \n",
    "    if i in missing_class_idx_test:\n",
    "        test_labels[i] = 0\n",
    "        \n",
    "for i in range(len(valid_final_labels)): \n",
    "    if i in missing_class_idx_val:\n",
    "        valid_final_labels[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.34      0.34      0.34       400\n",
      "          1       0.39      0.35      0.37       218\n",
      "          2       0.49      0.48      0.49       199\n",
      "          4       0.40      0.41      0.40       174\n",
      "          5       0.32      0.32      0.32       198\n",
      "          6       0.52      0.60      0.56       178\n",
      "\n",
      "avg / total       0.40      0.40      0.40      1367\n",
      "\n",
      "0.400877834674\n"
     ]
    }
   ],
   "source": [
    "print classification_report(test_labels, predicted_test_class)\n",
    "print accuracy_score(test_labels, predicted_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[134  89  36   0  32  65  44]\n",
      "400\n",
      "134.0 400\n",
      "0.335\n"
     ]
    }
   ],
   "source": [
    "print np.bincount(sorted(predicted_test_class[unseen_class_indices]))\n",
    "print sum(np.bincount(sorted(predicted_test_class[unseen_class_indices])))\n",
    "print float(np.bincount(sorted(predicted_test_class[unseen_class_indices]))[0]), \\\n",
    "    sum(np.isin(test_labels, missing_class).astype(int))\n",
    "print float(np.bincount(sorted(predicted_test_class[unseen_class_indices]))[0])/sum(np.isin(test_labels, \\\n",
    "                                                                                            missing_class).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3113, 450)\n",
      "(1094, 450)\n",
      "(1367, 450)\n",
      "Size of the train dataframe: (3113, 451)\n",
      "Size of the test dataframe: (1094, 451)\n",
      "Size of the test dataframe: (1367, 451)\n",
      "3113\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_final_vectors = np.array(train_final_vectors)\n",
    "valid_final_vectors=np.array(valid_final_vectors)\n",
    "test_vectors=np.array(test_vectors)\n",
    "print train_final_vectors.shape\n",
    "print valid_final_vectors.shape\n",
    "print test_vectors.shape\n",
    "\n",
    "\n",
    "X = train_final_vectors\n",
    "y = train_final_labels\n",
    "\n",
    "X_val = valid_final_vectors\n",
    "y_val = valid_final_labels\n",
    "\n",
    "X_test = test_vectors\n",
    "y_test = test_labels\n",
    "\n",
    "feat_cols = [ 'col'+str(i) for i in range(X.shape[1]) ]\n",
    "feat_cols_val = [ 'col'+str(i) for i in range(X_val.shape[1]) ]\n",
    "feat_cols_test = [ 'col'+str(i) for i in range(X_test.shape[1]) ]\n",
    "\n",
    "df = pd.DataFrame(X,columns=feat_cols)\n",
    "df['label'] = y\n",
    "df['label'] = df['label'].apply(lambda i: str(i))\n",
    "\n",
    "\n",
    "df_val = pd.DataFrame(X_val,columns=feat_cols_val)\n",
    "df_val['label'] = y_val\n",
    "df_val['label'] = df_val['label'].apply(lambda i: str(i))\n",
    "\n",
    "df_test = pd.DataFrame(X_test,columns=feat_cols_test)\n",
    "df_test['label'] = y_test\n",
    "df_test['label'] = df_test['label'].apply(lambda i: str(i))\n",
    "\n",
    "\n",
    "X, y = None, None\n",
    "print 'Size of the train dataframe: {}'.format(df.shape)\n",
    "print 'Size of the test dataframe: {}'.format(df_val.shape)\n",
    "print 'Size of the test dataframe: {}'.format(df_test.shape)\n",
    "\n",
    "N = df.shape[0]\n",
    "print N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Latent Semantic Analysis for dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using LSA\n",
      "done in 0.272237s\n",
      "Explained variance of the SVD step: 73%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "print(\"Performing dimensionality reduction using LSA\")\n",
    "t0 = time()\n",
    "svd = TruncatedSVD(n_components=20)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "lsa_result = lsa.fit_transform(df[feat_cols].values)\n",
    "lsa_val = lsa.fit_transform(df_val[feat_cols].values)\n",
    "lsa_test = lsa.transform(df_test[feat_cols_test].values)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also try PCA for dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variation per principal component: [ 0.24445201  0.08690181  0.07065607  0.04380071  0.03468903  0.03187359\n",
      "  0.02855609  0.02674412  0.01959448  0.01768235  0.01561487  0.01189199\n",
      "  0.01125397  0.01092048  0.01027324  0.0092663   0.00740557  0.00679036\n",
      "  0.00623175  0.00588426]\n",
      "Explained variation All components: 0.700483067892\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=20)\n",
    "pca_result = pca.fit_transform(df[feat_cols].values)\n",
    "\n",
    "df['pca-one'] = pca_result[:,0]\n",
    "df['pca-two'] = pca_result[:,1] \n",
    "#df['pca-three'] = pca_result[:,2]\n",
    "#df['pca-four'] = pca_result[:,3]\n",
    "#df['pca-five'] = pca_result[:,4]\n",
    "\n",
    "\n",
    "print 'Explained variation per principal component: {}'.format(pca.explained_variance_ratio_)\n",
    "print 'Explained variation All components: {}'.format(sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We will use LCA as it capture higher percentage of variation **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Gaussian Mixture Models of varying components and covariance matrix sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import operator\n",
    "from scipy import linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import scipy\n",
    "\n",
    "from sklearn import mixture\n",
    "print(__doc__)\n",
    "\n",
    "\n",
    "def GMM_fits(input_data=lsa_result, max_comp=21):\n",
    "    # Number of samples per component\n",
    "    n_samples = N\n",
    "    # Generate random sample, two components\n",
    "    np.random.seed(0)\n",
    "    X = np.array(input_data)\n",
    "    #X=np.array(pca_result_5)\n",
    "    print X.shape\n",
    "\n",
    "    bic_2 = {}\n",
    "    n_components_range = range(max_comp, max_comp+1)\n",
    "    cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "    aic_dict = {}\n",
    "    bic_dict = {}\n",
    "    lowest_bic = np.infty\n",
    "    lowest_aic = np.infty\n",
    "\n",
    "    for cv_type in cv_types:\n",
    "\n",
    "        bic = []\n",
    "        aic = []\n",
    "        for n_components in n_components_range:\n",
    "            # Fit a Gaussian mixture with EM\n",
    "            gmm = mixture.GaussianMixture(n_components=n_components,\n",
    "                                          covariance_type=cv_type, \n",
    "                                          reg_covar =1e-3,\n",
    "                                          random_state=0, \n",
    "                                          init_params='kmeans',\n",
    "                                          max_iter = 1350)\n",
    "            gmm.fit(X)\n",
    "            bic.append(gmm.bic(X))\n",
    "            aic.append(gmm.aic(X))\n",
    "            bic_2[cv_type+\"-\" + str(n_components)] = gmm.bic(X)\n",
    "            if bic[-1] < lowest_bic:\n",
    "                lowest_bic = bic[-1]\n",
    "                best_gmm = gmm\n",
    "            if aic[-1] < lowest_aic:\n",
    "                lowest_aic = aic[-1]\n",
    "\n",
    "        aic_dict[cv_type] = aic\n",
    "        bic_dict[cv_type] = bic\n",
    "\n",
    "    clf = best_gmm\n",
    "    sorted_bic_2 = sorted(bic_2.items(), key=operator.itemgetter(1))\n",
    "    return clf, bic_dict, aic_dict, sorted_bic_2, lowest_bic, lowest_aic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Plot AIC/BIC for model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_selection_criteria(best_model, aic_dict, bic_dict, information_criteria = 'bic', max_comp = 21):\n",
    "    n_components_range = range(1,max_comp)\n",
    "    if information_criteria == 'bic':\n",
    "        for cv in bic_dict.keys():\n",
    "            plt.plot(n_components_range, bic_dict[cv], label=str(cv))\n",
    "            #plt.plot(n_components, [m.aic(X) for m in models], label='AIC')\n",
    "            plt.legend(loc=cv)\n",
    "            plt.xlabel('n_components_range')\n",
    "        plt.title(\"BIC plot\")\n",
    "    else:\n",
    "        for cv in aic_dict.keys():\n",
    "            plt.plot(n_components_range, aic_dict[cv], label=str(cv))\n",
    "            #plt.plot(n_components, [m.aic(X) for m in models], label='AIC')\n",
    "            plt.legend(loc=cv)\n",
    "            plt.xlabel('n_components_range')\n",
    "        plt.title(\"AIC plot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3113, 20)\n"
     ]
    }
   ],
   "source": [
    "num_train_class = 5\n",
    "num_unseen_class =1\n",
    "best_lsa_model,lsa_bic_d,lsa_aic_d,sorted_lsa_bic,lowest_lsa_aic,lowest_lsa_bic = GMM_fits(input_data=lsa_result, \n",
    "                                                                                            max_comp=num_train_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the lowest AIC/BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest_BIC -107389.407284\n",
      "Lowest_AIC -100415.39042\n",
      "Best GMM Model Parameters based on BIC <bound method GaussianMixture.get_params of GaussianMixture(covariance_type='full', init_params='kmeans', max_iter=1350,\n",
      "        means_init=None, n_components=5, n_init=1, precisions_init=None,\n",
      "        random_state=0, reg_covar=0.001, tol=0.001, verbose=0,\n",
      "        verbose_interval=10, warm_start=False, weights_init=None)>\n"
     ]
    }
   ],
   "source": [
    "print \"Lowest_BIC\", lowest_lsa_bic\n",
    "print \"Lowest_AIC\", lowest_lsa_aic  \n",
    "print \"Best GMM Model Parameters based on BIC\", best_lsa_model.get_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction using the lowest BIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.123\n",
      "Completeness: 0.130\n",
      "V-measure: 0.127\n",
      "Adjusted Rand-Index: 0.080\n",
      "Silhouette Coefficient: 0.078\n",
      "fowlkes_mallows_score: 0.280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  450.,     0.,   287.,     0.,     0.,   845.,     0.,   531.,\n",
       "            0.,  1000.]),\n",
       " array([ 0. ,  0.4,  0.8,  1.2,  1.6,  2. ,  2.4,  2.8,  3.2,  3.6,  4. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1084ed3c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_cluster = best_lsa_model.predict(lsa_result)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(df['label'], predicted_cluster))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(df['label'], predicted_cluster))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(df['label'], predicted_cluster))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(df['label'], predicted_cluster))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(lsa_result, predicted_cluster, sample_size=1000))\n",
    "print (\"fowlkes_mallows_score: %.3f\"\n",
    "      % metrics.fowlkes_mallows_score(df['label'], predicted_cluster))\n",
    "plt.hist(predicted_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_comp(predicted_labels,actual_labels):\n",
    "    Cluster_ids = {}\n",
    "    for i in predicted_labels:\n",
    "        Cluster_ids[i] = (predicted_labels==i).nonzero()[0]\n",
    "\n",
    "    targets = np.array(actual_labels)\n",
    "    #print Cluster_ids\n",
    "    for label in Cluster_ids.keys():\n",
    "        #print type(label)\n",
    "        idx = Cluster_ids[label]\n",
    "        print \"Cluster Number\", str(label), \"Composition\", np.bincount(targets[idx])\n",
    "        print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Number 0 Composition [  0  69  65   0 120  85 111]\n",
      "\n",
      "\n",
      "Cluster Number 1 Composition [  0   2   5   0  17   9 254]\n",
      "\n",
      "\n",
      "Cluster Number 2 Composition [  0 117 259   0 139 235  95]\n",
      "\n",
      "\n",
      "Cluster Number 3 Composition [  0 191 189   0  45  83  23]\n",
      "\n",
      "\n",
      "Cluster Number 4 Composition [  0 220 102   0 341 223 114]\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print print_cluster_comp(predicted_labels=predicted_cluster, actual_labels=train_final_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba_val_labels = best_lsa_model.predict_proba(lsa_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.12812607e-01   4.24622229e-15   1.41315625e-37   8.71873930e-02\n",
      "    1.96091586e-10]\n",
      " [  1.10174789e-08   2.91360800e-14   1.06684165e-27   4.40229753e-06\n",
      "    9.99995587e-01]\n",
      " [  5.21074466e-02   1.06639360e-13   1.12839200e-09   9.46469218e-01\n",
      "    1.42333451e-03]\n",
      " ..., \n",
      " [  1.84576370e-08   1.43093535e-19   7.59612285e-14   9.99731695e-01\n",
      "    2.68286688e-04]\n",
      " [  9.99810216e-01   6.26628157e-15   4.32025369e-51   1.89784114e-04\n",
      "    2.35235028e-18]\n",
      " [  9.74661213e-01   1.84006319e-23   5.73772188e-28   4.81023840e-03\n",
      "    2.05285482e-02]]\n",
      "[ 1.  1.  1.  1.  1.]\n",
      "[[  9.12812607e-01   4.24622229e-15   1.41315625e-37   8.71873930e-02\n",
      "    1.96091586e-10]\n",
      " [  1.10174789e-08   2.91360800e-14   1.06684165e-27   4.40229753e-06\n",
      "    9.99995587e-01]\n",
      " [  5.21074466e-02   1.06639360e-13   1.12839200e-09   9.46469218e-01\n",
      "    1.42333451e-03]\n",
      " ..., \n",
      " [  1.84576370e-08   1.43093535e-19   7.59612285e-14   9.99731695e-01\n",
      "    2.68286688e-04]\n",
      " [  9.99810216e-01   6.26628157e-15   4.32025369e-51   1.89784114e-04\n",
      "    2.35235028e-18]\n",
      " [  9.74661213e-01   1.84006319e-23   5.73772188e-28   4.81023840e-03\n",
      "    2.05285482e-02]]\n"
     ]
    }
   ],
   "source": [
    "print pred_proba_val_labels\n",
    "val_gmm_class_max_prob_lists = np.array([max(pred_proba_val_labels[:,val]) for val in range(len(pred_proba_val_labels[0]))])\n",
    "val_gmm_class_min_prob_lists = np.array([min(pred_proba_val_labels[:,val]) for val in range(len(pred_proba_val_labels[0]))])\n",
    "val_gmm_delta = val_gmm_class_max_prob_lists - val_gmm_class_min_prob_lists\n",
    "\n",
    "print val_gmm_delta\n",
    "# Normalized for test vector\n",
    "scaled_gmm_val_class_probs = np.divide(pred_proba_val_labels,val_gmm_delta)\n",
    "print scaled_gmm_val_class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f10842e15d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.amax(pred_proba_val_labels, axis = 1)\n",
    "#print pred_proba_test_labels[:,0]\n",
    "plt.figure(figsize = (12, 8))\n",
    "n, bins, patches = plt.hist(x, 50)\n",
    "plt.title(\"max prob dist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9.99683389e-01   4.28810659e-04   1.51844833e-09   9.98795574e-01\n",
      "   8.11170017e-01]\n",
      "[[  9.12812607e-01   4.24622229e-15   1.41315625e-37   8.71873930e-02\n",
      "    1.96091586e-10]\n",
      " [  1.10174789e-08   2.91360800e-14   1.06684165e-27   4.40229753e-06\n",
      "    9.99995587e-01]\n",
      " [  5.21074466e-02   1.06639360e-13   1.12839200e-09   9.46469218e-01\n",
      "    1.42333451e-03]\n",
      " ..., \n",
      " [  1.84576370e-08   1.43093535e-19   7.59612285e-14   9.99731695e-01\n",
      "    2.68286688e-04]\n",
      " [  9.99810216e-01   6.26628157e-15   4.32025369e-51   1.89784114e-04\n",
      "    2.35235028e-18]\n",
      " [  9.74661213e-01   1.84006319e-23   5.73772188e-28   4.81023840e-03\n",
      "    2.05285482e-02]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 0]\n",
      " ..., \n",
      " [0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[ 0  1  2  4  5  6  8  9 10 12]\n"
     ]
    }
   ],
   "source": [
    "class_threshold = 85.0\n",
    "val_gmm_class_prob_percentile_cutoff = np.array([np.percentile(scaled_gmm_val_class_probs[:,val], class_threshold) \n",
    "                                     for val in range(len(scaled_gmm_val_class_probs[0]))])\n",
    "\n",
    "print val_gmm_class_prob_percentile_cutoff\n",
    "\n",
    "gmm_val_class_preds = np.greater_equal(scaled_gmm_val_class_probs,val_gmm_class_prob_percentile_cutoff).astype(int)\n",
    "\n",
    "print scaled_gmm_val_class_probs\n",
    "# Predict the test label based on percental\n",
    "# If a data is below 80% for prob of all class, it belongs to open class\n",
    "val_gmm_valid_class_probs = np.multiply(scaled_gmm_val_class_probs, gmm_val_class_preds)\n",
    "val_gmm_valid_class = np.greater_equal(np.ceil(val_gmm_valid_class_probs),1).astype(int)\n",
    "print val_gmm_valid_class\n",
    "val_gmm_predicted_multinomial = np.multiply(val_gmm_valid_class, np.unique(train_final_labels))\n",
    "print np.unique(np.sum(val_gmm_predicted_multinomial, axis=1))\n",
    "gmm_predicted_val_class = np.max(val_gmm_predicted_multinomial,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[314 165 144   0 155 151 165] [352 164 141   0 109 164 164]\n",
      "1094 1094\n"
     ]
    }
   ],
   "source": [
    "print np.bincount(valid_final_labels), np.bincount(gmm_predicted_val_class)\n",
    "print np.sum(np.bincount(valid_final_labels)), np.sum(np.bincount(gmm_predicted_val_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Unseen Class Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composition of gmm_predicted_valid_class for true unseen class indices [112  25   8   0  56  73  40]\n",
      "314\n"
     ]
    }
   ],
   "source": [
    "print \"Composition of gmm_predicted_valid_class for true unseen class indices\", np.bincount(sorted(gmm_predicted_val_class[missing_class_idx_val]))\n",
    "print sum(np.bincount(sorted(gmm_predicted_val_class[missing_class_idx_val])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.36      0.32      0.34       352\n",
      "          1       0.16      0.16      0.16       164\n",
      "          2       0.08      0.09      0.08       141\n",
      "          4       0.04      0.06      0.05       109\n",
      "          5       0.13      0.12      0.12       164\n",
      "          6       0.11      0.11      0.11       164\n",
      "\n",
      "avg / total       0.19      0.18      0.18      1094\n",
      "\n",
      "[[112  73  42  55  47  23]\n",
      " [ 25  27  19  41  28  24]\n",
      " [  8   9  12  14  17  81]\n",
      " [ 56   8  16   6  10  13]\n",
      " [ 73  29  19  18  19   6]\n",
      " [ 40  19  36  21  30  18]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "print classification_report(gmm_predicted_val_class, valid_final_labels)\n",
    "print confusion_matrix(gmm_predicted_val_class, valid_final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.318181818182 0.356687898089 0.336336336336\n"
     ]
    }
   ],
   "source": [
    "def calculate_unseen_class_f1score(pred_class, true_class, unseen_class_id):\n",
    "    predicted_zero_ind = (pred_class==unseen_class_id).nonzero()[0]\n",
    "    predicted_nonzero_ind = (pred_class > unseen_class_id).nonzero()[0]\n",
    "    #print np.bincount(true_class[predicted_zero_ind])[0]\n",
    "    TP = np.bincount(true_class[predicted_zero_ind])[0]\n",
    "    FP =  sum(np.bincount(true_class[predicted_zero_ind])) - TP\n",
    "    FN =  np.bincount(true_class[predicted_nonzero_ind])[0]\n",
    "    #print TP, FP, FN\n",
    "    unseen_class_precision = float(TP)/(TP+FP)\n",
    "    unseen_class_recall = float(TP)/(TP+FN)\n",
    "    unseen_class_f1 = 2*unseen_class_precision*unseen_class_recall/(unseen_class_precision+unseen_class_recall)\n",
    "    #print unseen_class_precision,unseen_class_recall,unseen_class_f1\n",
    "    return unseen_class_precision, unseen_class_recall,unseen_class_f1\n",
    "    \n",
    "pr,re,f1 = calculate_unseen_class_f1score(gmm_predicted_val_class, np.array(valid_final_labels), 0)\n",
    "print pr,re,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  67 F1 Score:  0.0062893081761\n",
      "Actual Unseen Class 314 Predicted Unseen Class 4\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  68 F1 Score:  0.0061919504644\n",
      "Actual Unseen Class 314 Predicted Unseen Class 9\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  69 F1 Score:  0.0180722891566\n",
      "Actual Unseen Class 314 Predicted Unseen Class 18\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  70 F1 Score:  0.0237388724036\n",
      "Actual Unseen Class 314 Predicted Unseen Class 23\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  71 F1 Score:  0.0293255131965\n",
      "Actual Unseen Class 314 Predicted Unseen Class 27\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  72 F1 Score:  0.0402298850575\n",
      "Actual Unseen Class 314 Predicted Unseen Class 34\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  73 F1 Score:  0.050139275766\n",
      "Actual Unseen Class 314 Predicted Unseen Class 45\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  74 F1 Score:  0.0537634408602\n",
      "Actual Unseen Class 314 Predicted Unseen Class 58\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  75 F1 Score:  0.0571428571429\n",
      "Actual Unseen Class 314 Predicted Unseen Class 71\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  76 F1 Score:  0.074812967581\n",
      "Actual Unseen Class 314 Predicted Unseen Class 87\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  77 F1 Score:  0.132387706856\n",
      "Actual Unseen Class 314 Predicted Unseen Class 109\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  78 F1 Score:  0.160356347439\n",
      "Actual Unseen Class 314 Predicted Unseen Class 135\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  79 F1 Score:  0.191489361702\n",
      "Actual Unseen Class 314 Predicted Unseen Class 156\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  80 F1 Score:  0.221327967807\n",
      "Actual Unseen Class 314 Predicted Unseen Class 183\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  81 F1 Score:  0.238461538462\n",
      "Actual Unseen Class 314 Predicted Unseen Class 206\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  82 F1 Score:  0.260869565217\n",
      "Actual Unseen Class 314 Predicted Unseen Class 238\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  83 F1 Score:  0.30220713073\n",
      "Actual Unseen Class 314 Predicted Unseen Class 275\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  84 F1 Score:  0.312599681021\n",
      "Actual Unseen Class 314 Predicted Unseen Class 313\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  85 F1 Score:  0.336336336336\n",
      "Actual Unseen Class 314 Predicted Unseen Class 352\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  86 F1 Score:  0.35360678925\n",
      "Actual Unseen Class 314 Predicted Unseen Class 393\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  87 F1 Score:  0.359731543624\n",
      "Actual Unseen Class 314 Predicted Unseen Class 431\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  88 F1 Score:  0.374522292994\n",
      "Actual Unseen Class 314 Predicted Unseen Class 471\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  89 F1 Score:  0.388955582233\n",
      "Actual Unseen Class 314 Predicted Unseen Class 519\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  90 F1 Score:  0.393592677346\n",
      "Actual Unseen Class 314 Predicted Unseen Class 560\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  91 F1 Score:  0.396963123644\n",
      "Actual Unseen Class 314 Predicted Unseen Class 608\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  92 F1 Score:  0.397119341564\n",
      "Actual Unseen Class 314 Predicted Unseen Class 658\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  93 F1 Score:  0.414467253177\n",
      "Actual Unseen Class 314 Predicted Unseen Class 709\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  94 F1 Score:  0.432282003711\n",
      "Actual Unseen Class 314 Predicted Unseen Class 764\n"
     ]
    }
   ],
   "source": [
    "best_threshold = 0.0\n",
    "best_f1_score=0.0\n",
    "for threshold in range(50,95):\n",
    "    try:\n",
    "        val_gmm_class_prob_percentile_cutoff = np.array([np.percentile(scaled_gmm_val_class_probs[:,val], float(threshold)) \n",
    "                                             for val in range(len(scaled_gmm_val_class_probs[0]))])\n",
    "\n",
    "        #print val_gmm_class_prob_percentile_cutoff\n",
    "\n",
    "        gmm_val_class_preds = np.greater_equal(scaled_gmm_val_class_probs,val_gmm_class_prob_percentile_cutoff).astype(int)\n",
    "\n",
    "        #print scaled_gmm_val_class_probs\n",
    "        # Predict the test label based on percental\n",
    "        # If a data is below 80% for prob of all class, it belongs to open class\n",
    "        val_gmm_valid_class_probs_dup = np.multiply(scaled_gmm_val_class_probs, gmm_val_class_preds)\n",
    "\n",
    "        val_gmm_valid_class_max_probs = np.max(val_gmm_valid_class_probs_dup, axis=1)\n",
    "        #print gmm_valid_class_max_probs\n",
    "        #print type(gmm_valid_class_probs_dup),  type(gmm_valid_class_max_probs)\n",
    "        val_temp = np.equal(val_gmm_valid_class_probs_dup , val_gmm_valid_class_max_probs.reshape(len(val_gmm_valid_class_max_probs),1))\n",
    "        val_gmm_valid_class_probs=np.multiply(val_gmm_valid_class_probs_dup,val_temp)\n",
    "        val_gmm_valid_class = np.greater_equal(np.ceil(val_gmm_valid_class_probs),1).astype(int)\n",
    "        #print val_gmm_valid_class\n",
    "        val_gmm_predicted_multinomial = np.multiply(val_gmm_valid_class, np.unique(train_final_labels))\n",
    "        #print np.unique(np.sum(val_gmm_predicted_multinomial, axis=1))\n",
    "        gmm_predicted_val_class = np.max(val_gmm_predicted_multinomial,axis=1)  \n",
    "        print np.unique(gmm_predicted_val_class), np.unique(valid_final_labels)\n",
    "        pr,re,f1 = calculate_unseen_class_f1score(gmm_predicted_val_class,valid_final_labels,0)\n",
    "        print \"Threshold: \",threshold, \"F1 Score: \", f1 \n",
    "        print \"Actual Unseen Class\", np.bincount(valid_final_labels)[0], \"Predicted Unseen Class\",np.bincount(gmm_predicted_val_class)[0]\n",
    "        # Set the threshold so that not unseen class volume is actual unseen class volume in validation set\n",
    "        unseen_class_ratio = float(np.bincount(gmm_predicted_val_class)[0])/np.bincount(valid_final_labels)[0]\n",
    "        #overall_F1_score = f1_score(gmm_predicted_val_class, valid_final_labels)\n",
    "        if f1 > best_f1_score and unseen_class_ratio < 1.1:\n",
    "\n",
    "            best_f1_score = f1\n",
    "            best_threshold = threshold\n",
    "    except:\n",
    "        print \"Threshold Too Low. No unseen class prediction\"\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba_test_labels = best_lsa_model.predict_proba(lsa_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.78720667e-01   6.20393341e-09   1.87229745e-17   1.09607529e-02\n",
      "    1.03185739e-02]\n",
      " [  1.11178754e-04   4.48379307e-14   2.62468739e-19   9.99807620e-01\n",
      "    8.12010722e-05]\n",
      " [  2.21268846e-27   3.15556053e-12   1.00000000e+00   7.65584638e-18\n",
      "    5.42257195e-13]\n",
      " ..., \n",
      " [  9.99996007e-01   6.34688768e-18   1.14870138e-38   3.99258106e-06\n",
      "    3.35793068e-10]\n",
      " [  2.40780149e-04   2.57547394e-16   1.79619590e-14   9.98939636e-01\n",
      "    8.19583748e-04]\n",
      " [  2.21268846e-27   3.15556053e-12   1.00000000e+00   7.65584638e-18\n",
      "    5.42257195e-13]]\n",
      "[ 1.  1.  1.  1.  1.]\n",
      "[[  9.78720667e-01   6.20393341e-09   1.87229745e-17   1.09607529e-02\n",
      "    1.03185739e-02]\n",
      " [  1.11178754e-04   4.48379307e-14   2.62468739e-19   9.99807620e-01\n",
      "    8.12010722e-05]\n",
      " [  2.21268846e-27   3.15556053e-12   1.00000000e+00   7.65584638e-18\n",
      "    5.42257195e-13]\n",
      " ..., \n",
      " [  9.99996007e-01   6.34688768e-18   1.14870138e-38   3.99258106e-06\n",
      "    3.35793068e-10]\n",
      " [  2.40780149e-04   2.57547394e-16   1.79619590e-14   9.98939636e-01\n",
      "    8.19583748e-04]\n",
      " [  2.21268846e-27   3.15556053e-12   1.00000000e+00   7.65584638e-18\n",
      "    5.42257195e-13]]\n"
     ]
    }
   ],
   "source": [
    "print pred_proba_test_labels\n",
    "gmm_class_max_prob_lists = np.array([max(pred_proba_test_labels[:,val]) for val in range(len(pred_proba_test_labels[0]))])\n",
    "gmm_class_min_prob_lists = np.array([min(pred_proba_test_labels[:,val]) for val in range(len(pred_proba_test_labels[0]))])\n",
    "gmm_delta = gmm_class_max_prob_lists - gmm_class_min_prob_lists\n",
    "\n",
    "print gmm_delta\n",
    "# Normalized for test vector\n",
    "scaled_gmm_test_class_probs = np.divide(pred_proba_test_labels,gmm_delta)\n",
    "print scaled_gmm_test_class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f108370df90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.amax(pred_proba_test_labels, axis = 1)\n",
    "#print pred_proba_test_labels[:,0]\n",
    "plt.figure(figsize = (12, 8))\n",
    "n, bins, patches = plt.hist(x, 50)\n",
    "plt.title(\"max prob dist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "[[ 0.          0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.99980762  0.        ]\n",
      " [ 0.          0.          1.          0.          0.        ]\n",
      " ..., \n",
      " [ 0.99999601  0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.99893964  0.        ]\n",
      " [ 0.          0.          1.          0.          0.        ]]\n",
      "[ 0.          0.99980762  1.         ...,  0.99999601  0.99893964  1.        ]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]\n",
      " ..., \n",
      " [1 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 1 0 0]]\n",
      "[0 1 2 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print best_threshold\n",
    "gmm_class_prob_percentile_cutoff = np.array([np.percentile(scaled_gmm_test_class_probs[:,val], best_threshold) \n",
    "                                     for val in range(len(scaled_gmm_test_class_probs[0]))])\n",
    "\n",
    "#print gmm_class_prob_percentile_cutoff\n",
    "\n",
    "gmm_test_class_preds = np.greater_equal(scaled_gmm_test_class_probs,gmm_class_prob_percentile_cutoff).astype(int)\n",
    "\n",
    "#print scaled_gmm_test_class_probs\n",
    "# Predict the test label based on percental\n",
    "# If a data is below 80% for prob of all class, it belongs to open class\n",
    "gmm_valid_class_probs_dup = np.multiply(scaled_gmm_test_class_probs, gmm_test_class_preds)\n",
    "print gmm_valid_class_probs_dup\n",
    "gmm_valid_class_max_probs = np.max(gmm_valid_class_probs_dup, axis=1)\n",
    "print gmm_valid_class_max_probs\n",
    "#print type(gmm_valid_class_probs_dup),  type(gmm_valid_class_max_probs)\n",
    "temp = np.equal(gmm_valid_class_probs_dup , gmm_valid_class_max_probs.reshape(len(gmm_valid_class_max_probs),1))\n",
    "gmm_valid_class_probs=np.multiply(gmm_valid_class_probs_dup,temp)\n",
    "gmm_valid_class = np.greater_equal(np.ceil(gmm_valid_class_probs),1).astype(int)\n",
    "print gmm_valid_class\n",
    "gmm_predicted_multinomial = np.multiply(gmm_valid_class, np.unique(train_final_labels))\n",
    "print np.unique(np.sum(gmm_predicted_multinomial, axis=1))\n",
    "gmm_predicted_test_class = np.max(gmm_predicted_multinomial,axis=1)\n",
    "\n",
    "# The true unseen class index\n",
    "#unseen_class_indices = np.where(np.isin(test_labels, missing_class))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400 218 199   0 174 198 178] [386 219 187   0 137 219 219]\n",
      "1367 1367\n"
     ]
    }
   ],
   "source": [
    "print np.bincount(test_labels), np.bincount(gmm_predicted_test_class)\n",
    "print np.sum(np.bincount(test_labels)), np.sum(np.bincount(gmm_predicted_test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[123  50  18  56 104  49]\n",
      " [ 75  39  18  22  32  32]\n",
      " [ 51  33  24  17  21  53]\n",
      " [ 57  43  18  14  25  17]\n",
      " [ 65  28  21  13  23  48]\n",
      " [ 15  26  88  15  14  20]]\n"
     ]
    }
   ],
   "source": [
    "print confusion_matrix(test_labels, gmm_predicted_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.32      0.31      0.31       400\n",
      "          1       0.18      0.18      0.18       218\n",
      "          2       0.13      0.12      0.12       199\n",
      "          4       0.10      0.08      0.09       174\n",
      "          5       0.11      0.12      0.11       198\n",
      "          6       0.09      0.11      0.10       178\n",
      "\n",
      "avg / total       0.18      0.18      0.18      1367\n",
      "\n",
      "0.17776152158\n"
     ]
    }
   ],
   "source": [
    "print classification_report(test_labels, gmm_predicted_test_class)\n",
    "print accuracy_score(test_labels, gmm_predicted_test_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseen Class Precision, Recall F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen Class Precision:  0.318652849741\n",
      "Unseen Class Recall:  0.3075\n",
      "Unseen Class F1 Score:  0.312977099237\n"
     ]
    }
   ],
   "source": [
    "print \"Unseen Class Precision: \", calculate_unseen_class_f1score(gmm_predicted_test_class,test_labels,0)[0]\n",
    "print \"Unseen Class Recall: \", calculate_unseen_class_f1score(gmm_predicted_test_class,test_labels,0)[1]\n",
    "print \"Unseen Class F1 Score: \", calculate_unseen_class_f1score(gmm_predicted_test_class,test_labels,0)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class_indices = (gmm_predicted_test_class==0).nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.057\n",
      "Completeness: 0.057\n",
      "V-measure: 0.057\n",
      "Adjusted Rand-Index: 0.039\n",
      "Silhouette Coefficient: 0.047\n",
      "fowlkes_mallows_score: 0.217\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(df_test['label'], gmm_predicted_test_class))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(df_test['label'], gmm_predicted_test_class))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(df_test['label'], gmm_predicted_test_class))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(df_test['label'], gmm_predicted_test_class))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(lsa_test, gmm_predicted_test_class, sample_size=1000))\n",
    "print (\"fowlkes_mallows_score: %.3f\"\n",
    "      % metrics.fowlkes_mallows_score(df_test['label'], gmm_predicted_test_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Clustering Unsee Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_class_indices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-84b8d7db6b89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted_unseen_class_lsa\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlsa_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_class_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_class_indices' is not defined"
     ]
    }
   ],
   "source": [
    "predicted_unseen_class_lsa= lsa_test[pred_class_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_unseen_class_lsa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-c793d5919a9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m models = [BayesianGaussianMixture(n_components, covariance_type=c, init_params='random').fit(predicted_unseen_class_lsa)\n\u001b[0;32m----> 6\u001b[0;31m           for c in c_var]\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_unseen_class_lsa' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "\n",
    "c_var = ['full', 'tied', 'diag', 'spherical']\n",
    "n_components=1\n",
    "models = [BayesianGaussianMixture(n_components, covariance_type=c, init_params='random').fit(predicted_unseen_class_lsa)\n",
    "          for c in c_var]\n",
    "\n",
    "for m in range(len(models)):\n",
    "    plt.title(\"Model with Covariance type: \" + str(c_var[m]))\n",
    "    plt.hist(models[m].predict(predicted_unseen_class_lsa))\n",
    "    pred_ls = models[m].predict(predicted_unseen_class_lsa)\n",
    "    print (\"fowlkes_mallows_score: %.3f\"\n",
    "      % metrics.fowlkes_mallows_score(pred_ls, orig_test_labels[pred_class_indices]))\n",
    "    plt.show()\n",
    "#gmm2_model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite Dirichlet Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BGMM_fits(input_data=lsa_result, cv_type='full', max_comp=21):\n",
    "    mod = BayesianGaussianMixture(n_components=max_comp, covariance_type='full', \n",
    "                                  weight_concentration_prior_type=\"dirichlet_process\", \n",
    "                                  reg_covar=0, init_params='random',\n",
    "        max_iter=1500, mean_precision_prior=.8,random_state=0)\n",
    "    X = np.array(input_data)\n",
    "    mod.fit(X)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_class = 5\n",
    "num_unseen_class = 2\n",
    "best_lsa_model = BGMM_fits(input_data=lsa_result, max_comp=num_train_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best IDP Model Parameters based on BIC <bound method BayesianGaussianMixture.get_params of BayesianGaussianMixture(covariance_prior=None, covariance_type='full',\n",
      "            degrees_of_freedom_prior=None, init_params='random',\n",
      "            max_iter=1500, mean_precision_prior=0.8, mean_prior=None,\n",
      "            n_components=5, n_init=1, random_state=0, reg_covar=0,\n",
      "            tol=0.001, verbose=0, verbose_interval=10, warm_start=False,\n",
      "            weight_concentration_prior=None,\n",
      "            weight_concentration_prior_type='dirichlet_process')>\n"
     ]
    }
   ],
   "source": [
    "print \"Best IDP Model Parameters based on BIC\", best_lsa_model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.051\n",
      "Completeness: 0.054\n",
      "V-measure: 0.053\n",
      "Adjusted Rand-Index: 0.044\n",
      "Silhouette Coefficient: 0.048\n",
      "fowlkes_mallows_score: 0.247\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 583.,    0.,  675.,    0.,    0.,  222.,    0.,  875.,    0.,  758.]),\n",
       " array([ 0. ,  0.4,  0.8,  1.2,  1.6,  2. ,  2.4,  2.8,  3.2,  3.6,  4. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f10171bf910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_cluster = best_lsa_model.predict(lsa_result)\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(df['label'], predicted_cluster))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(df['label'], predicted_cluster))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(df['label'], predicted_cluster))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(df['label'], predicted_cluster))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(lsa_result, predicted_cluster, sample_size=1000))\n",
    "print (\"fowlkes_mallows_score: %.3f\"\n",
    "      % metrics.fowlkes_mallows_score(df['label'], predicted_cluster))\n",
    "plt.hist(predicted_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cluster_comp(predicted_labels,actual_labels):\n",
    "    Cluster_ids = {}\n",
    "    for i in predicted_labels:\n",
    "        Cluster_ids[i] = (predicted_labels==i).nonzero()[0]\n",
    "\n",
    "    targets = np.array(actual_labels)\n",
    "    #print Cluster_ids\n",
    "    for label in Cluster_ids.keys():\n",
    "        #print type(label)\n",
    "        idx = Cluster_ids[label]\n",
    "        print \"Cluster Number\", str(label), \"Composition\", np.bincount(targets[idx])\n",
    "        print \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Number 0 Composition [  0  69  64   0 106  80 264]\n",
      "\n",
      "\n",
      "Cluster Number 1 Composition [  0 113 167   0  79 179 137]\n",
      "\n",
      "\n",
      "Cluster Number 2 Composition [ 0 29 49  0 61 41 42]\n",
      "\n",
      "\n",
      "Cluster Number 3 Composition [  0 176 122   0 286 188 103]\n",
      "\n",
      "\n",
      "Cluster Number 4 Composition [  0 212 218   0 130 147  51]\n",
      "\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print print_cluster_comp(predicted_labels=predicted_cluster, actual_labels=train_final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.90839611e-001   0.00000000e+000   0.00000000e+000   3.14344802e-207\n",
      "    9.16038917e-003]\n",
      " [  4.66885412e-008   3.53397900e-174   0.00000000e+000   9.64832524e-001\n",
      "    3.51674296e-002]\n",
      " [  1.69432095e-002   2.53206687e-217   0.00000000e+000   1.01850660e-014\n",
      "    9.83056791e-001]\n",
      " ..., \n",
      " [  7.18164973e-007   0.00000000e+000   0.00000000e+000   6.60560627e-085\n",
      "    9.99999282e-001]\n",
      " [  9.99985442e-001   0.00000000e+000   0.00000000e+000   1.03832709e-216\n",
      "    1.45579797e-005]\n",
      " [  2.31505372e-001   0.00000000e+000   0.00000000e+000   7.48310662e-133\n",
      "    7.68494628e-001]]\n",
      "[  1.00000000e+00   4.12767735e-15   1.00000000e+00   1.00000000e+00\n",
      "   1.00000000e+00]\n",
      "[[  9.90839611e-001   0.00000000e+000   0.00000000e+000   3.14344802e-207\n",
      "    9.16038917e-003]\n",
      " [  4.66885412e-008   8.56166484e-160   0.00000000e+000   9.64832524e-001\n",
      "    3.51674296e-002]\n",
      " [  1.69432095e-002   6.13436239e-203   0.00000000e+000   1.01850660e-014\n",
      "    9.83056791e-001]\n",
      " ..., \n",
      " [  7.18164973e-007   0.00000000e+000   0.00000000e+000   6.60560627e-085\n",
      "    9.99999282e-001]\n",
      " [  9.99985442e-001   0.00000000e+000   0.00000000e+000   1.03832709e-216\n",
      "    1.45579797e-005]\n",
      " [  2.31505372e-001   0.00000000e+000   0.00000000e+000   7.48310662e-133\n",
      "    7.68494628e-001]]\n"
     ]
    }
   ],
   "source": [
    "pred_proba_val_labels = best_lsa_model.predict_proba(lsa_val)\n",
    "print pred_proba_val_labels\n",
    "val_gmm_class_max_prob_lists = np.array([max(pred_proba_val_labels[:,val]) for val in range(len(pred_proba_val_labels[0]))])\n",
    "val_gmm_class_min_prob_lists = np.array([min(pred_proba_val_labels[:,val]) for val in range(len(pred_proba_val_labels[0]))])\n",
    "val_gmm_delta = val_gmm_class_max_prob_lists - val_gmm_class_min_prob_lists\n",
    "\n",
    "print val_gmm_delta\n",
    "# Normalized for test vector\n",
    "scaled_bgmm_val_class_probs = np.divide(pred_proba_val_labels,val_gmm_delta)\n",
    "print scaled_bgmm_val_class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f10169e07d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.amax(pred_proba_val_labels, axis = 1)\n",
    "#print pred_proba_test_labels[:,0]\n",
    "plt.figure(figsize = (12, 8))\n",
    "n, bins, patches = plt.hist(x, 50)\n",
    "plt.title(\"max prob dist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e+000   8.28281407e-109   0.00000000e+000   5.05576302e-013\n",
      "   9.99979646e-001]\n",
      "[[  9.90839611e-001   0.00000000e+000   0.00000000e+000   3.14344802e-207\n",
      "    9.16038917e-003]\n",
      " [  4.66885412e-008   8.56166484e-160   0.00000000e+000   9.64832524e-001\n",
      "    3.51674296e-002]\n",
      " [  1.69432095e-002   6.13436239e-203   0.00000000e+000   1.01850660e-014\n",
      "    9.83056791e-001]\n",
      " ..., \n",
      " [  7.18164973e-007   0.00000000e+000   0.00000000e+000   6.60560627e-085\n",
      "    9.99999282e-001]\n",
      " [  9.99985442e-001   0.00000000e+000   0.00000000e+000   1.03832709e-216\n",
      "    1.45579797e-005]\n",
      " [  2.31505372e-001   0.00000000e+000   0.00000000e+000   7.48310662e-133\n",
      "    7.68494628e-001]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " ..., \n",
      " [0 0 0 0 1]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 15 17]\n"
     ]
    }
   ],
   "source": [
    "class_threshold = 85.0\n",
    "val_bgmm_class_prob_percentile_cutoff = np.array([np.percentile(scaled_bgmm_val_class_probs[:,val], class_threshold) \n",
    "                                     for val in range(len(scaled_bgmm_val_class_probs[0]))])\n",
    "\n",
    "print val_bgmm_class_prob_percentile_cutoff\n",
    "\n",
    "bgmm_val_class_preds = np.greater_equal(scaled_bgmm_val_class_probs,val_bgmm_class_prob_percentile_cutoff).astype(int)\n",
    "\n",
    "print scaled_bgmm_val_class_probs\n",
    "# Predict the test label based on percental\n",
    "# If a data is below 80% for prob of all class, it belongs to open class\n",
    "val_bgmm_valid_class_probs = np.multiply(scaled_bgmm_val_class_probs, bgmm_val_class_preds)\n",
    "val_bgmm_valid_class = np.greater_equal(np.ceil(val_bgmm_valid_class_probs),1).astype(int)\n",
    "print val_bgmm_valid_class\n",
    "val_bgmm_predicted_multinomial = np.multiply(val_bgmm_valid_class, np.unique(train_final_labels))\n",
    "print np.unique(np.sum(val_bgmm_predicted_multinomial, axis=1))\n",
    "bgmm_predicted_val_class = np.max(val_bgmm_predicted_multinomial,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[314 165 144   0 155 151 165] [764  66  66   0  66  66  66]\n",
      "1094 1094\n"
     ]
    }
   ],
   "source": [
    "print np.bincount(valid_final_labels), np.bincount(gmm_predicted_val_class)\n",
    "print np.sum(np.bincount(valid_final_labels)), np.sum(np.bincount(gmm_predicted_val_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Unseen Class Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composition of bgmm_predicted_valid_class for true unseen class indices [140  17   7   0  16  45  89]\n",
      "314\n"
     ]
    }
   ],
   "source": [
    "print \"Composition of bgmm_predicted_valid_class for true unseen class indices\", np.bincount(sorted(bgmm_predicted_val_class[missing_class_idx_val]))\n",
    "print sum(np.bincount(sorted(gmm_predicted_val_class[missing_class_idx_val])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.45      0.25      0.32       556\n",
      "          1       0.08      0.09      0.09       159\n",
      "          2       0.03      0.22      0.06        23\n",
      "          4       0.01      0.03      0.02        60\n",
      "          5       0.20      0.23      0.21       132\n",
      "          6       0.02      0.02      0.02       164\n",
      "\n",
      "avg / total       0.27      0.18      0.21      1094\n",
      "\n",
      "[[140  97  80  98  74  67]\n",
      " [ 17  14  18  24  21  65]\n",
      " [  7   2   5   4   3   2]\n",
      " [ 16   4  12   2  10  16]\n",
      " [ 45  18  11  16  30  12]\n",
      " [ 89  30  18  11  13   3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, precision_score, recall_score, f1_score\n",
    "print classification_report(bgmm_predicted_val_class, valid_final_labels)\n",
    "print confusion_matrix(bgmm_predicted_val_class, valid_final_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.251798561151 0.445859872611 0.32183908046\n"
     ]
    }
   ],
   "source": [
    "def calculate_unseen_class_f1score(pred_class, true_class, unseen_class_id):\n",
    "    predicted_zero_ind = (pred_class==unseen_class_id).nonzero()[0]\n",
    "    predicted_nonzero_ind = (pred_class >unseen_class_id).nonzero()[0]\n",
    "    #print np.bincount(true_class[predicted_zero_ind])[0]\n",
    "    TP = np.bincount(true_class[predicted_zero_ind])[0]\n",
    "    FP =  sum(np.bincount(true_class[predicted_zero_ind])) - TP\n",
    "    FN =  np.bincount(true_class[predicted_nonzero_ind])[0]\n",
    "    #print TP, FP, FN\n",
    "    unseen_class_precision = float(TP)/(TP+FP)\n",
    "    unseen_class_recall = float(TP)/(TP+FN)\n",
    "    unseen_class_f1 = 2*unseen_class_precision*unseen_class_recall/(unseen_class_precision+unseen_class_recall)\n",
    "    #print unseen_class_precision,unseen_class_recall,unseen_class_f1\n",
    "    return unseen_class_precision, unseen_class_recall,unseen_class_f1\n",
    "    \n",
    "pr,re,f1 = calculate_unseen_class_f1score(bgmm_predicted_val_class,valid_final_labels,0)\n",
    "print pr,re,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold Too Low. No unseen class prediction\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  54 F1 Score:  0.01875\n",
      "Actual Unseen Class 314 Predicted Unseen Class 6\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  55 F1 Score:  0.0363636363636\n",
      "Actual Unseen Class 314 Predicted Unseen Class 16\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  56 F1 Score:  0.0414201183432\n",
      "Actual Unseen Class 314 Predicted Unseen Class 24\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  57 F1 Score:  0.0462427745665\n",
      "Actual Unseen Class 314 Predicted Unseen Class 32\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  58 F1 Score:  0.0569800569801\n",
      "Actual Unseen Class 314 Predicted Unseen Class 37\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  59 F1 Score:  0.0767123287671\n",
      "Actual Unseen Class 314 Predicted Unseen Class 51\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  60 F1 Score:  0.0906666666667\n",
      "Actual Unseen Class 314 Predicted Unseen Class 61\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  61 F1 Score:  0.103896103896\n",
      "Actual Unseen Class 314 Predicted Unseen Class 71\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  62 F1 Score:  0.110552763819\n",
      "Actual Unseen Class 314 Predicted Unseen Class 84\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  63 F1 Score:  0.127139364303\n",
      "Actual Unseen Class 314 Predicted Unseen Class 95\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  64 F1 Score:  0.141176470588\n",
      "Actual Unseen Class 314 Predicted Unseen Class 111\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  65 F1 Score:  0.145124716553\n",
      "Actual Unseen Class 314 Predicted Unseen Class 127\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  66 F1 Score:  0.149122807018\n",
      "Actual Unseen Class 314 Predicted Unseen Class 142\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  67 F1 Score:  0.15644820296\n",
      "Actual Unseen Class 314 Predicted Unseen Class 159\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  68 F1 Score:  0.159836065574\n",
      "Actual Unseen Class 314 Predicted Unseen Class 174\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  69 F1 Score:  0.167664670659\n",
      "Actual Unseen Class 314 Predicted Unseen Class 187\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  70 F1 Score:  0.180076628352\n",
      "Actual Unseen Class 314 Predicted Unseen Class 208\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  71 F1 Score:  0.186813186813\n",
      "Actual Unseen Class 314 Predicted Unseen Class 232\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  72 F1 Score:  0.187943262411\n",
      "Actual Unseen Class 314 Predicted Unseen Class 250\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  73 F1 Score:  0.204081632653\n",
      "Actual Unseen Class 314 Predicted Unseen Class 274\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  74 F1 Score:  0.21568627451\n",
      "Actual Unseen Class 314 Predicted Unseen Class 298\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  75 F1 Score:  0.216560509554\n",
      "Actual Unseen Class 314 Predicted Unseen Class 314\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  76 F1 Score:  0.22629969419\n",
      "Actual Unseen Class 314 Predicted Unseen Class 340\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  77 F1 Score:  0.237188872621\n",
      "Actual Unseen Class 314 Predicted Unseen Class 369\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  78 F1 Score:  0.245363766049\n",
      "Actual Unseen Class 314 Predicted Unseen Class 387\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  79 F1 Score:  0.260387811634\n",
      "Actual Unseen Class 314 Predicted Unseen Class 408\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  80 F1 Score:  0.274562584118\n",
      "Actual Unseen Class 314 Predicted Unseen Class 429\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  81 F1 Score:  0.291287386216\n",
      "Actual Unseen Class 314 Predicted Unseen Class 455\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  82 F1 Score:  0.297604035309\n",
      "Actual Unseen Class 314 Predicted Unseen Class 479\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  83 F1 Score:  0.303921568627\n",
      "Actual Unseen Class 314 Predicted Unseen Class 502\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  84 F1 Score:  0.313539192399\n",
      "Actual Unseen Class 314 Predicted Unseen Class 528\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  85 F1 Score:  0.32183908046\n",
      "Actual Unseen Class 314 Predicted Unseen Class 556\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  86 F1 Score:  0.333333333333\n",
      "Actual Unseen Class 314 Predicted Unseen Class 580\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  87 F1 Score:  0.340195016251\n",
      "Actual Unseen Class 314 Predicted Unseen Class 609\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  88 F1 Score:  0.349372384937\n",
      "Actual Unseen Class 314 Predicted Unseen Class 642\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  89 F1 Score:  0.354969574037\n",
      "Actual Unseen Class 314 Predicted Unseen Class 672\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  90 F1 Score:  0.362745098039\n",
      "Actual Unseen Class 314 Predicted Unseen Class 706\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  91 F1 Score:  0.36660268714\n",
      "Actual Unseen Class 314 Predicted Unseen Class 728\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  92 F1 Score:  0.367424242424\n",
      "Actual Unseen Class 314 Predicted Unseen Class 742\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  93 F1 Score:  0.382189239332\n",
      "Actual Unseen Class 314 Predicted Unseen Class 764\n",
      "[0 1 2 4 5 6] [0 1 2 4 5 6]\n",
      "Threshold:  94 F1 Score:  0.394950405771\n",
      "Actual Unseen Class 314 Predicted Unseen Class 795\n",
      "Best Threshold:  76\n",
      "Best F1 Score:  0.22629969419\n"
     ]
    }
   ],
   "source": [
    "best_threshold = 0.0\n",
    "best_f1_score=0.0\n",
    "for threshold in range(50,95):\n",
    "    try:\n",
    "        val_bgmm_class_prob_percentile_cutoff = np.array([np.percentile(scaled_bgmm_val_class_probs[:,val], float(threshold)) \n",
    "                                             for val in range(len(scaled_bgmm_val_class_probs[0]))])\n",
    "\n",
    "        #print val_gmm_class_prob_percentile_cutoff\n",
    "\n",
    "        bgmm_val_class_preds = np.greater_equal(scaled_bgmm_val_class_probs,val_bgmm_class_prob_percentile_cutoff).astype(int)\n",
    "\n",
    "        #print scaled_gmm_val_class_probs\n",
    "        # Predict the test label based on percental\n",
    "        # If a data is below 80% for prob of all class, it belongs to open class\n",
    "        val_bgmm_valid_class_probs_dup = np.multiply(scaled_bgmm_val_class_probs, bgmm_val_class_preds)\n",
    "\n",
    "        val_bgmm_valid_class_max_probs = np.max(val_bgmm_valid_class_probs_dup, axis=1)\n",
    "        #print gmm_valid_class_max_probs\n",
    "        #print type(gmm_valid_class_probs_dup),  type(gmm_valid_class_max_probs)\n",
    "        val_temp = np.equal(val_bgmm_valid_class_probs_dup , val_bgmm_valid_class_max_probs.reshape(len(val_bgmm_valid_class_max_probs),1))\n",
    "        val_bgmm_valid_class_probs=np.multiply(val_bgmm_valid_class_probs_dup,val_temp)\n",
    "        val_bgmm_valid_class = np.greater_equal(np.ceil(val_bgmm_valid_class_probs),1).astype(int)\n",
    "        #print val_gmm_valid_class\n",
    "        val_bgmm_predicted_multinomial = np.multiply(val_bgmm_valid_class, np.unique(train_final_labels))\n",
    "        #print np.unique(np.sum(val_gmm_predicted_multinomial, axis=1))\n",
    "        bgmm_predicted_val_class = np.max(val_bgmm_predicted_multinomial,axis=1)  \n",
    "        print np.unique(bgmm_predicted_val_class), np.unique(valid_final_labels)\n",
    "        pr,re,f1 = calculate_unseen_class_f1score(bgmm_predicted_val_class,valid_final_labels,0)\n",
    "        print \"Threshold: \",threshold, \"F1 Score: \", f1 \n",
    "        print \"Actual Unseen Class\", np.bincount(valid_final_labels)[0], \"Predicted Unseen Class\",np.bincount(bgmm_predicted_val_class)[0]\n",
    "        # Set the threshold so that not unseen class volume is actual unseen class volume in validation set\n",
    "        unseen_class_ratio = float(np.bincount(bgmm_predicted_val_class)[0])/np.bincount(valid_final_labels)[0]\n",
    "        #overall_F1_score = f1_score(gmm_predicted_val_class, valid_final_labels)\n",
    "        if f1 > best_f1_score and unseen_class_ratio < 1.1:\n",
    "\n",
    "            best_f1_score = f1\n",
    "            best_threshold = threshold\n",
    "    except:\n",
    "        print \"Threshold Too Low. No unseen class prediction\"\n",
    "        \n",
    "print \"Best Threshold: \", best_threshold\n",
    "print \"Best F1 Score: \", best_f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.99999994e-001   0.00000000e+000   0.00000000e+000   4.63276312e-048\n",
      "    6.17761979e-009]\n",
      " [  8.20994693e-002   0.00000000e+000   0.00000000e+000   6.39285453e-124\n",
      "    9.17900531e-001]\n",
      " [  1.02866559e-032   4.12767735e-015   1.00000000e+000   1.07444741e-024\n",
      "    1.28551530e-038]\n",
      " ..., \n",
      " [  9.99999832e-001   0.00000000e+000   0.00000000e+000   1.39879966e-030\n",
      "    1.67924802e-007]\n",
      " [  3.13343613e-006   5.57581288e-177   0.00000000e+000   5.37584208e-017\n",
      "    9.99996867e-001]\n",
      " [  1.02866559e-032   4.12767735e-015   1.00000000e+000   1.07444741e-024\n",
      "    1.28551530e-038]]\n",
      "[  1.00000000e+00   5.29235987e-14   1.00000000e+00   9.99999993e-01\n",
      "   1.00000000e+00]\n",
      "[[  9.99999994e-001   0.00000000e+000   0.00000000e+000   4.63276315e-048\n",
      "    6.17761979e-009]\n",
      " [  8.20994693e-002   0.00000000e+000   0.00000000e+000   6.39285458e-124\n",
      "    9.17900531e-001]\n",
      " [  1.02866559e-032   7.79931343e-002   1.00000000e+000   1.07444741e-024\n",
      "    1.28551530e-038]\n",
      " ..., \n",
      " [  9.99999832e-001   0.00000000e+000   0.00000000e+000   1.39879967e-030\n",
      "    1.67924802e-007]\n",
      " [  3.13343613e-006   1.05355891e-163   0.00000000e+000   5.37584212e-017\n",
      "    9.99996867e-001]\n",
      " [  1.02866559e-032   7.79931343e-002   1.00000000e+000   1.07444741e-024\n",
      "    1.28551530e-038]]\n"
     ]
    }
   ],
   "source": [
    "pred_proba_test_labels = best_lsa_model.predict_proba(lsa_test)\n",
    "print pred_proba_test_labels\n",
    "bgmm_class_max_prob_lists = np.array([max(pred_proba_test_labels[:,val]) for val in range(len(pred_proba_test_labels[0]))])\n",
    "bgmm_class_min_prob_lists = np.array([min(pred_proba_test_labels[:,val]) for val in range(len(pred_proba_test_labels[0]))])\n",
    "bgmm_delta = bgmm_class_max_prob_lists - bgmm_class_min_prob_lists\n",
    "\n",
    "print bgmm_delta\n",
    "# Normalized for test vector\n",
    "scaled_bgmm_test_class_probs = np.divide(pred_proba_test_labels,bgmm_delta)\n",
    "print scaled_bgmm_test_class_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1016782fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.amax(pred_proba_test_labels, axis = 1)\n",
    "#print pred_proba_test_labels[:,0]\n",
    "plt.figure(figsize = (12, 8))\n",
    "n, bins, patches = plt.hist(x, 50)\n",
    "plt.title(\"max prob dist\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n",
      "[[  9.99999994e-01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00]\n",
      " [  0.00000000e+00   7.79931343e-02   1.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00]\n",
      " ..., \n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00]\n",
      " [  0.00000000e+00   0.00000000e+00   0.00000000e+00   5.37584212e-17\n",
      "    9.99996867e-01]\n",
      " [  0.00000000e+00   7.79931343e-02   1.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00]]\n",
      "[ 0.99999999  0.          1.         ...,  0.          0.99999687  1.        ]\n",
      "[[1 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 1 0 0]\n",
      " ..., \n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 1]\n",
      " [0 0 1 0 0]]\n",
      "[0 1 2 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "print best_threshold\n",
    "bgmm_class_prob_percentile_cutoff = np.array([np.percentile(scaled_bgmm_test_class_probs[:,val], best_threshold) \n",
    "                                     for val in range(len(scaled_bgmm_test_class_probs[0]))])\n",
    "\n",
    "#print gmm_class_prob_percentile_cutoff\n",
    "\n",
    "bgmm_test_class_preds = np.greater_equal(scaled_bgmm_test_class_probs,bgmm_class_prob_percentile_cutoff).astype(int)\n",
    "\n",
    "#print scaled_gmm_test_class_probs\n",
    "# Predict the test label based on percental\n",
    "# If a data is below 80% for prob of all class, it belongs to open class\n",
    "bgmm_valid_class_probs_dup = np.multiply(scaled_bgmm_test_class_probs, bgmm_test_class_preds)\n",
    "print bgmm_valid_class_probs_dup\n",
    "bgmm_valid_class_max_probs = np.max(bgmm_valid_class_probs_dup, axis=1)\n",
    "print bgmm_valid_class_max_probs\n",
    "#print type(gmm_valid_class_probs_dup),  type(gmm_valid_class_max_probs)\n",
    "temp = np.equal(bgmm_valid_class_probs_dup , bgmm_valid_class_max_probs.reshape(len(bgmm_valid_class_max_probs),1))\n",
    "bgmm_valid_class_probs=np.multiply(bgmm_valid_class_probs_dup,temp)\n",
    "bgmm_valid_class = np.greater_equal(np.ceil(bgmm_valid_class_probs),1).astype(int)\n",
    "print bgmm_valid_class\n",
    "bgmm_predicted_multinomial = np.multiply(bgmm_valid_class, np.unique(train_final_labels))\n",
    "print np.unique(np.sum(bgmm_predicted_multinomial, axis=1))\n",
    "bgmm_predicted_test_class = np.max(bgmm_predicted_multinomial,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400 218 199   0 174 198 178] [416 328  34   0  77 185 327]\n",
      "1367 1367\n",
      "[[111  61   3  23  57 145]\n",
      " [ 64  46   2  13  37  56]\n",
      " [ 74  44  15  13  20  33]\n",
      " [ 61  53   1  10  15  34]\n",
      " [ 65  34   6   7  46  40]\n",
      " [ 41  90   7  11  10  19]]\n"
     ]
    }
   ],
   "source": [
    "print np.bincount(test_labels), np.bincount(bgmm_predicted_test_class)\n",
    "print np.sum(np.bincount(test_labels)), np.sum(np.bincount(bgmm_predicted_test_class))\n",
    "print confusion_matrix(test_labels, bgmm_predicted_test_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.32      0.31      0.31       400\n",
      "          1       0.18      0.18      0.18       218\n",
      "          2       0.13      0.12      0.12       199\n",
      "          4       0.10      0.08      0.09       174\n",
      "          5       0.11      0.12      0.11       198\n",
      "          6       0.09      0.11      0.10       178\n",
      "\n",
      "avg / total       0.18      0.18      0.18      1367\n",
      "\n",
      "0.17776152158\n"
     ]
    }
   ],
   "source": [
    "print classification_report(test_labels, gmm_predicted_test_class)\n",
    "print accuracy_score(test_labels, gmm_predicted_test_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unseen Class Precision, Recall F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unseen Class Precision:  0.266826923077\n",
      "Unseen Class Recall:  0.2775\n",
      "Unseen Class F1 Score:  0.272058823529\n"
     ]
    }
   ],
   "source": [
    "print \"Unseen Class Precision: \", calculate_unseen_class_f1score(bgmm_predicted_test_class,test_labels,0)[0]\n",
    "print \"Unseen Class Recall: \", calculate_unseen_class_f1score(bgmm_predicted_test_class,test_labels,0)[1]\n",
    "print \"Unseen Class F1 Score: \", calculate_unseen_class_f1score(bgmm_predicted_test_class,test_labels,0)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.057\n",
      "Completeness: 0.057\n",
      "V-measure: 0.057\n",
      "Adjusted Rand-Index: 0.039\n",
      "Silhouette Coefficient: 0.044\n",
      "fowlkes_mallows_score: 0.217\n"
     ]
    }
   ],
   "source": [
    "pred_class_indices = (gmm_predicted_test_class==0).nonzero()[0]\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(df_test['label'], gmm_predicted_test_class))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(df_test['label'], gmm_predicted_test_class))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(df_test['label'], gmm_predicted_test_class))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(df_test['label'], gmm_predicted_test_class))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(lsa_test, gmm_predicted_test_class, sample_size=1000))\n",
    "print (\"fowlkes_mallows_score: %.3f\"\n",
    "      % metrics.fowlkes_mallows_score(df_test['label'], gmm_predicted_test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
